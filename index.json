[{"content":"The Rocket crate provides a lot of web server functionality that is simple to use. It compares well to other web-server backend libraries like Flask in Python. Currently, no Rust framework even registers on the Hacker Rank surveys of favorite web frameworks. That will change. Rocket is easy to use with great documentation and has over 21k stars on GitHub. This is a brief but detailed introduction to using Rocket, by the end of this post you should have a basic understanding of how to get, post, and put using Rocket.\nTo get started with Rocket you need to add it to your Cargo.toml file.\n[package] name = \u0026#34;my-first-rocket\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [dependencies] rocket = \u0026#34;0.5.0-rc.3\u0026#34; Adding methods and routes to your main.rs file.\n#[macro_use] extern crate rocket; #[get(\u0026#34;/\u0026#34;)] fn index() -\u0026gt; \u0026amp;\u0026#39;static str { \u0026#34;Hello, world!\u0026#34; } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![index]) } Then run cargo run and enter http://127.0.0.1:8000/ into your browser. You should see \u0026ldquo;Hello, world!\u0026rdquo;.\nFor comparison, the Python example coded in Flask is comparable to the Rust version coded in Rocket.\nfrom flask import Flask app = Flask(__name__) @app.route(\u0026#34;/\u0026#34;) def index(): return \u0026#34;Hello World!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: app.run() This is about the same amount of code as needed in Python\u0026rsquo;s Flask. With some extra code going towards specifying types and brackets. For a small investment in additional syntax, you gain all the benefits of type safety, no garbage collection, and not having to install Python\u0026rsquo;s runtimes to deploy this code. Creating and deploying the Rocket version is as simple as running cargo build --release and then copying the binary to the deployment location.\nThe get was pretty easy. Next, I will demonstrate the post and put to get a better understanding of what it is like to code using Rocket. This is where life gets a little more interesting. One of the selling points of Rust is \u0026ldquo;fearless concurrency\u0026rdquo; which is not even a thing in frameworks like Flask. Flask also has different goals than Rocket, it doesn\u0026rsquo;t pretend to be highly concurrent. Rocket like Rust wants to make use of all your CPU cores. That comes with the cost of learning how to write concurrent code in Rust.\nIn this demo, I will be replicating a database in a hashmap data structure using HashMap\u0026lt;u32, String\u0026gt;. However, since we need to make this thread safe it will need to be wrapped in a mutex and held in a Rocket State like this.\nuse rocket::tokio::sync::Mutex; use rocket::State; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; The State reference requires a lifetime which is what \u0026lt;'r\u0026gt; syntax represents.\nSo far we have only defined the types that make up the database. We have not instantiated it yet. Later, when we wire up the routes we will instantiate the hashmap and add it as a managed resource. This will make it available to functions using Rocket\u0026rsquo;s annotations.\n#[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![index]) .manage(DataHashMap::new(HashMap::new())) } JSON payloads tend to be ubiquitous on the Web whether it is for REST payloads or other purposes. In this post, we will examine two ways to represent deserialized JSON within a struct. First, the obvious approach. It has the benefit of being easy to understand at the cost of constantly having to own the string when you need to modify the value. The entire point of using Rust is to be efficient, so this is not the most idiomatic approach.\nuse rocket::serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } The more idiomatic approach is to use the Cow smart pointer since you don\u0026rsquo;t have to think about if the value is just borrowed or owned. However, you will need to consider lifetimes in functions that use it. A small penalty to pay for the performance gain. However, depending on your use case. If your endpoint is under a light load, the additional memory spent may be worth it, but then you probably don\u0026rsquo;t need Rust or Rocket either.\nuse rocket::serde::{Deserialize, Serialize}; use std::borrow::Cow; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } First, I will implement the post, put, and get using non-reference versions of the struct.\n#[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![new]) // Added the route for the \u0026#34;new\u0026#34; function .manage(DataHashMap::new(HashMap::new())) } The function new is annotated with the relative path, the format of the payload, JSON in this case and the variable that will hold the payload.\n#[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] Since Rocket is designed to be concurrent we are going to use async functions. The function receives the JSON payload which is deserialized by Serde which is what data: Json\u0026lt;Data\u0026gt; does. The final argument passes in the database which is nothing more than a hashmap wrapped in a mutex that has its state managed by Rocket as declared earlier. The code database: Database\u0026lt;'_\u0026gt; may look odd, this is where we have to use lifetime notation to assist the Rust borrow checker because the type Database is a reference type. This ensures that the reference will remain valid through the life of the function call. The \u0026lt;'_\u0026gt; represents an anonymous lifetime. When you see \u0026lt;\u0026rsquo;_\u0026gt; in a type annotation or a function signature, it means that the reference has a specific, but unnamed, lifetime. The compiler will infer the actual lifetime based on the context where the reference is used. The function then returns a tuple of its status and a detailed message in a returned JSON payload.\nasync fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) Since our database is wrapped in a mutex we must obtain a lock to it, so the state does not change while we attempt to retrieve a value for a given key in the hashmap. The code also checks for a unique key and does not allow duplicate entries for a given id.\nlet mut db = database.lock().await; // Get lock on the hashmap if db.contains_key(\u0026amp;data.id) { // Check for duplicate id! return ( // If a duplicate return with approriate status message Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } Once we are sure we don\u0026rsquo;t have a duplicate entry in our database we do the insert and return success results to the client.\ndb.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) At this point, the complete working example.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![new]) .manage(DataHashMap::new(HashMap::new())) } To retrieve the data stored we need to implement the get function and add its route to the mount. Also, notice the use of pattern matching which makes the code more compact and easier to comprehend.\n#[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new]) .manage(DataHashMap::new(HashMap::new())) } Now we can insert data and return data. Using the VS Code plugin Rest Client\nPOST http://127.0.0.1:8000/insert HTTP/1.1 content-type: application/json { \u0026#34;id\u0026#34;: 1, \u0026#34;value\u0026#34;: \u0026#34;NEW DATA\u0026#34; } To get the data back, in a browser enter http://127.0.0.1:8000/getdata/1 or use the Rest Client.\nGET http://127.0.0.1:8000/getdata/1 HTTP/1.1 The results are:\nHTTP/1.1 202 Accepted content-type: application/json server: Rocket x-content-type-options: nosniff x-frame-options: SAMEORIGIN permissions-policy: interest-cohort=() content-length: 15 date: Tue, 04 Jul 2023 12:58:05 GMT { \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; } The update function using a put will be more of the same.\n#[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } The complete example.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } Next, let\u0026rsquo;s refactor this code to make it more idiomatic Rust. We will replace:\n#[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } with\nuse rocket::serde::{Deserialize, Serialize}; use std::borrow::Cow; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } The post will also be refactored to use pattern matching.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::borrow::Cow; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; match db.contains_key(\u0026amp;data.id) { false =\u0026gt; { db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } true =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ), } } #[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } In the final refactored version we now take advantage of smart pointers, however, now we need to deal with more lifetimes.\nasync fn new(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) // data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt; has lifetime notation async fn update(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) // data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt; has lifetime notation Conclusion Rocket is simple and it shares a lot in common with other libraries for different languages. Becoming productive in Rust is possible once you overcome the learning curve of the borrow checker and learn some concurrent programming skills. If you already have those skills, using Rocket is just as simple as using frameworks in more dynamic languages. Also, does not take six months to become proficient in Rust, you can become productive in two months.\nRust\u0026rsquo;s ecosystem for web development is growing. It doesn\u0026rsquo;t have complete batteries-included (opinionated) frameworks like Django but that also mirrors the Rust standard runtime which is not prescriptive and encourages the use of the crates that meet your needs. Rocket is one of many frameworks under active development such as Actix Web and Axum(https://github.com/tokio-rs/axum). The \u0026ldquo;Are we web yet?\u0026rdquo; is a great resource for learning about the state of web development in Rust.\nOther thoughts While not readily apparent from this intro to Rocket, managing memory as efficiently as possible will have an impact on performance and cost. Does it matter? It depends on your company\u0026rsquo;s goals and what its values are. Most companies don\u0026rsquo;t care about writing efficient code for the sake of writing efficient code. Most companies value developer productivity more than server costs which is why garbage-collected languages like Python, Go, and Java remain popular. For that class of company, sticking with Python, Javascript, etc. will serve them well. On the other side of the spectrum are a class of companies where scaling out has significant impacts on costs or in some cases all out performance matters to them. Making use of all your cores in Rust is a much lower-risk activity compared to C/C++. A real-world example of this was Cloudflare\u0026rsquo;s rewriting of an NGINX module in Rust.\nThe source code can be found here. \n","permalink":"https://kurtfehlhauer.com/posts/rusts-rocket/","summary":"The Rocket crate provides a lot of web server functionality that is simple to use. It compares well to other web-server backend libraries like Flask in Python. Currently, no Rust framework even registers on the Hacker Rank surveys of favorite web frameworks. That will change. Rocket is easy to use with great documentation and has over 21k stars on GitHub. This is a brief but detailed introduction to using Rocket, by the end of this post you should have a basic understanding of how to get, post, and put using Rocket.","title":"Rust's Rocket"},{"content":"Rust is gaining increasing recognition as the most loved language in the Stack Overflow developer surveys. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by The RedMonk Programming Language Rankings: January 2023. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.\nThough Rust is undeniably elegant, it diverges from other languages in one significant respect – memory management. Unlike C/C++ or Java/Python Rust does not rely on manual memory allocation or garbage collection. It employs a unique concept called \u0026lsquo;memory ownership\u0026rsquo;, which may initially seem bewildering to engineers more familiar with garbage-collected languages like C#, Java, Python, and Scala, where manual memory management is a foreign concept. Those coming from C/C++ may have an easier journey using Rust because they have a headstart with pointers and references. Becoming comfortable with Rust\u0026rsquo;s approach to memory management is an integral part of the learning curve. While many will find the continuous barking of errors by the Rust compiler to be annoying, those error messages will correct many subtle bugs that other languages just ignore. My advice to those keen on exploring Rust is to persevere through these challenges. Embracing Rust will refine your programming skills. Your perseverance will be rewarded with applications that have no buffer overflows, safe concurrency, and predictable latency.\nThus far, JVM languages like Java and Scala have ruled the big data engineering space, underpinning frameworks like Apache Flink and Spark. The JVM dominance in big data systems is a legacy of infrastructure laid down by Java in the Hadoop and MapReduce era. The JDK big data ecosystem is mature and battle-hardened and may have reached its pinnacle. However, as data sizes skyrocket, the downsides of JVM, such as significant memory consumption and garbage collection pauses, become increasingly evident, especially at scale. These issues trigger multi-level repercussions, leading to increased cloud costs, latency, failure rates, and other problems that data engineers grapple with daily. Rust can help you solve those issues.\nThe foundation for Rust\u0026rsquo;s entry into the data engineering world is already in place. There are available libraries for dealing with Arrow, Parquet, and JSON parsing, high-performing caching libraries like Mocha, and the under-development Polars dataframe library that\u0026rsquo;s written in Rust but can be utilized in Python. Asynchronous runtimes like Tokio and Rayon enable multi-core CPU usage.\nFurthermore, the introduction of Delta Lake RS provides a pathway to incorporate Rust into existing Deltalakes. Powered by the Apache Arrow platform, Delta Lake RS opens opportunities to transfer some workloads away from Spark, although much work is still needed for broader adoption.\nLet\u0026rsquo;s get started by transforming JSON to parquet, a common use case. The JDK has strong support for that use case with even stronger support in Scala. See the previous post Two Scala Libraries Every Data Engineer Should Know. Rust has a framework called Serde which can serialize and deserialize various data formats to and from Rust structs. Serde will handle the conversion between JSON and structs but not Parquet. For that, we will need to use the Parquet and Arrow crates.\nGiven this JSON:\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} I want the data stored in the Parquet file that I\u0026rsquo;m creating to have the following schema:\nmessage arrow_schema { REQUIRED BYTE_ARRAY VIN (STRING); REQUIRED BYTE_ARRAY make (STRING); REQUIRED BYTE_ARRAY model (STRING); REQUIRED INT32 year (INTEGER(16,false)); REQUIRED BYTE_ARRAY owner (STRING); OPTIONAL BOOLEAN isRegistered; } First, we start by creating a Rust struct that we can deserialize the JSON into.\nuse serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize, Debug)] #[allow(non_snake_case)] struct Vechicle { VIN: String, make: String, model: String, year: u16, owner: String, isRegistered: Option\u0026lt;bool\u0026gt;, } Using the attributes Serialize and Deserialize invoke compile time macros that create the boilerplate code to serialize and deserialize between structs and JSON. Rust uses particular casings that the compiler enforces through warnings. The VIN and the isRegistered fields are not in snake case, the attribute allow(non_snake_case) is used to suppress the warning.\nNext, we read a file called vehicles.json containing the above JSON. Deserializing the JSON into Rust structs straight forward.\nlet v: Vechicle = serde_json::from_str(\u0026amp;js)?; Reading in the entire file:\nlet mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = Vec::new(); if let Ok(lines) = read_lines(\u0026#34;vehicles.json\u0026#34;) { for line in lines { if let Ok(js) = line { let v: Vechicle = serde_json::from_str(\u0026amp;js)?; vehicles.push(v); } } } Writing to a Parquet file in Rust currently involves some boilerplate code that is accomplished in three steps. Much of this boilerplate could be abstracted away using Rust macros. Macros are beyond the scope of this post.\n Create arrays for each column.  use arrow::array::{ArrayRef, StringArray}; let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); Create a RecordBatch to hold the column arrays. Notice the use of the Arc( ‘Arc’ stands for ‘Atomically Reference Counted’) type. It is a thread-safe reference-counting pointer. Those new to Rust will eventually want to obtain a cursory understanding of how threading works in Rust. For now, it\u0026rsquo;s just an implementation detail.  use arrow::record_batch::RecordBatch; let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), ... ]) .unwrap(); Use the ArrowWriter to write the record batch to a file. The compression is also set in this step.  use parquet::arrow::arrow_writer::ArrowWriter; use parquet::basic::Compression; let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); The complete write function.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use parquet::basic::Compression; use parquet::file::properties::WriterProperties; use std::sync::Arc; fn write(vehicles: Vec\u0026lt;Vechicle\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let makes = StringArray::from( vehicles .iter() .map(|v| v.make.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let models = StringArray::from( vehicles .iter() .map(|v| v.model.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let years = UInt16Array::from(vehicles.iter().map(|v| v.year).collect::\u0026lt;Vec\u0026lt;u16\u0026gt;\u0026gt;()); let owners = StringArray::from( vehicles .iter() .map(|v| v.owner.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let registrations = BooleanArray::from(vehicles.iter().map(|v| v.isRegistered).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;()); let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), (\u0026#34;make\u0026#34;, Arc::new(makes) as ArrayRef), (\u0026#34;model\u0026#34;, Arc::new(models) as ArrayRef), (\u0026#34;year\u0026#34;, Arc::new(years) as ArrayRef), (\u0026#34;owner\u0026#34;, Arc::new(owners) as ArrayRef), (\u0026#34;isRegistered\u0026#34;, Arc::new(registrations) as ArrayRef), ]) .unwrap(); let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); Ok(()) } Reading a Parquet file is much the same but in reverse.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use arrow::record_batch::RecordBatch; use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder; #[allow(non_snake_case)] fn read() -\u0026gt; Result\u0026lt;()\u0026gt; { let file = File::open(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let arrow_reader = ParquetRecordBatchReaderBuilder::try_new(file).unwrap(); let record_batch_reader = arrow_reader.build().unwrap(); let mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = vec![]; for maybe_batch in record_batch_reader { let record_batch = maybe_batch.unwrap(); let VIN = record_batch .column(0) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let make = record_batch .column(1) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let model = record_batch .column(2) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let year = record_batch .column(3) .as_any() .downcast_ref::\u0026lt;UInt16Array\u0026gt;() .unwrap(); let owner = record_batch .column(4) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let isRegistered = record_batch .column(5) .as_any() .downcast_ref::\u0026lt;BooleanArray\u0026gt;(); for i in 0..record_batch.num_rows() { vehicles.push(Vechicle { VIN: VIN.value(i).to_string(), make: make.value(i).to_string(), model: model.value(i).to_string(), year: year.value(i), owner: owner.value(i).to_string(), isRegistered: isRegistered.map(|a| a.value(i)), }); } } In conclusion, transforming data from JSON to Parquet is a straightforward process requiring only a few Rust crates. As the Rust ecosystem is further developed I expect data engineering tasks to become more commonplace. In my subsequent posts, I\u0026rsquo;ll delve into the nitty-gritty of using other libraries for various data engineering tasks. Stay tuned for more insights into the exciting possibilities that Rust brings to the table.\nThe full source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/exploring-rust-for-data-engineering-part-1/","summary":"Rust is gaining increasing recognition as the most loved language in the Stack Overflow developer surveys. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by The RedMonk Programming Language Rankings: January 2023. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.","title":"Exploring Rust For Data Engineering Part 1"},{"content":"As data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable. In this article, I\u0026rsquo;m going to demonstrate how two Scala libraries can work together to convert JSON to Parquet without using Spark, zio-json and Parquet4s. A working knowledge of functional programming in Scala and Zio is helpful, but not required.\nFor this example, we are going to use JSON data which reflects vehicle ownership and if the vehicle is registered or not. The conversion from JSON to Parquet is a two-step process.\n Load the JSON into a Scala case class using zio-json. Create the Parquet file from the Scala case class.  Our data, notice that we have a field that is optional, isRegistered.\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} Next, we construct a case class called Vehicle. The optional field, isRegistered, is an Option type. In the companion object, we create an implicit decoder.\nimport zio.json.* final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } Then we can decode the JSON with the fromJson[type] function.\ndef decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) In order to turn our JSON into Scala case classes it is a matter of passing each JSON string to decodeJson function.\nimport zio.* import zio.json.* import zio.Console._ object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) yield () def run = program The final step is to take our List of case classes and create a Parquet file from them. We need to do some Hadoop configuration, but you won\u0026rsquo;t need to have the full Hadoop ecosystem installed for this example to work.\nimport com.github.mjakubowski84.parquet4s.{ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet yield () At this point, we will have successfully transformed JSON to Parquet.\nWe can verify our write step was successful by reading back the parquet file that was just created. We read the Parquet functionally and handle closing file resources gracefully. Then we display the contents in the vehicle case classes.\nimport com.github.mjakubowski84.parquet4s.ParquetReader def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) def program = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet yield () The output will look like this.\nVehicle(1A123,foo,bar,2002,John Doe,Some(true)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,Some(false)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,None) Not let us put it all together. So that we can:\n Read the JSON into Scala case classes. Write the case classes out as a parquet file. Display the contents of the parquet file. Perform some cleanup.  import zio.* import zio.json.* import zio.Console._ import java.io.File import com.github.mjakubowski84.parquet4s.{ParquetReader, ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) val cleanUp = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) _ \u0026lt;- ZIO.attemptBlocking(new File(s\u0026#34;${wd}/vehicles.parquet\u0026#34;).delete) yield () def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet _ \u0026lt;- cleanUp yield () def run = program In this recipe, we used two libraries zio-json and Parquet4s to easily create Parquet files from JSON. The source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/two-scala-libraries-every-data-engineer-should-know/","summary":"As data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable.","title":"Two Scala Libraries Every Data Engineer Should Know"},{"content":"Resume/CV Kurt Fehlhauer email: kfehlhauer@pm.me\nSummary I am a software engineering leader with a track record of delivering systems in complex environments.\nExperience 2/2022 - Present\nChief Data Architect Stellantis: Remote As the Chief Data Architect at Stellantis, I spearheaded the creation and currently manage the data platform team, encompassing data, DevOps, and software engineers. My key focus lies in overseeing all vehicle telemetry data for distinguished Stellantis brands, including Maserati and Ram trucks while collaborating with partners to deliver robust data products at an enterprise scale.\nKey Accomplishments and Responsibilities:\n Set the strategic roadmap and vision for enterprise-scale data engineering utilizing Databricks Spark and Delta Lake Orchestrated the consolidation of legacy data systems from two distinct car companies: FCA LLC and PSA Group Constructed a multi-petabyte data platform leveraging Airflow, Kubernetes, and Spark Pioneered the introduction of Spark streaming to process billions of daily records Forecasted and managed a multimillion-dollar data platform budget Successfully migrated data from various on-premise data sources to the cloud Regularly report on status and progress directly to C-suite leadership Champion IT innovation to enhance data accessibility and utility through thoughtful design and technology choices Partnered with data privacy officers to implement effective anonymization strategies, protecting personally identifiable information (PII) Established infrastructure supporting machine learning to enrich the vehicle cockpit experience Continuously exploring technologies like Rust to improve system performance, lower costs, and environmental impact Cultivated a robust engineering culture committed to continuous learning and skill expansion Collaborated with partners to foster innovations in engineering and marketing   5/2019 – 2/2022\nSenior Manager, ETL Activision Publishing: Remote As the Senior Manager of ETL at Activision Publishing, I spearheaded a diverse international team of data engineers. We developed data products and extended support to data scientists and analytics analysts across the company, and I also collaborated with data scientists to execute feature engineering.\nKey Contributions and Responsibilities:\n Managed comprehensive silver/gold level data for the Call of Duty franchise Led a successful migration of ETL processes from Amazon Web Services (AWS) to Google Cloud Platform (GCP), enhancing platform compatibility Developed a Spark-based ETL framework that empowered self-service ETL for many use cases Migrated and normalized a legacy ETL system to streamline various Call of Duty title data, achieving a 50% reduction in data availability wait time Introduced Astronomer Airflow to ease the deployment of Airflow across various data teams at Activision Consolidated and simplified Activision\u0026rsquo;s big data platforms from Qubole/Hive/Presto and Redshift to Databricks Promoted the adoption of MLFlow for data science workflows within the Activision Data Science community Enhanced Spark functionality through user-defined functions and integrated third-party data with Activision\u0026rsquo;s game data for improved insights Developed a third-party data ingestion framework using Cats/ZIO, Circe, and http4s on Kubernetes Implemented GDPR, CCPA, and other privacy measures within the data lake to ensure compliance with data protection regulations Partnered with Activision game studios to convert game artifact data into formats queryable in SparkSQL, retooling small data tools to function within Activision\u0026rsquo;s big data frameworks Improved Diversity, Equity, and Inclusion (DEI) efforts by widening the hiring pipeline for potential candidates Mentored staff in Airflow, Kubernetes, Scala, and Spark, enhancing team capabilities Managed multimillion-dollar contracts, ensuring efficient utilization of resources and delivery of various data products   6/2014 – 4/2019\nLead Database Architect Activision Publishing: Boulder, CO/Remote In my role as Lead Data Engineer at Activision\u0026rsquo;s Central Data Products organization, I championed significant advancements in our analytics, model-building capabilities, and game design support. I introduced the use of Apache Airflow and Spark company-wide, significantly impacting our data management and analytical capabilities.\nKey Contributions and Responsibilities:\n Collaborating with game developers to optimize gameplay elements such as vehicles and weapons in Call of Duty: Black Ops IV through data-driven analytics Collaborated with data scientists to construct models that enhanced gameplay performance Boosted the Play of The Match (PTOM) simulation\u0026rsquo;s performance by 20% Pioneered the introduction and training of Apache Spark at Activision, enabling more efficient data analysis Developed Spark extensions in Scala to handle encoded data Managed the hiring process for data engineers and data scientists, contributing to a highly skilled team Spearheaded the company-wide adoption of Airflow and its migration from DCOS to Kubernetes Oversaw multiple Hive, Presto, and Spark clusters within Qubole Formulated best practices for utilizing big data technologies such as Presto and Apache Spark Established a data pipeline service for capturing prelaunch and beta data for Call of Duty titles Designed ETL processes using Python Pandas dataframes for data ingestion for the Chinese version of Call of Duty Worked closely with Tencent to meet the data needs for Call of Duty Online Implemented a Vertica columnar database to support data from Call of Duty Online   1/2013 – 6/2014\nSenior Consultant FICO: Remote In my role as a team lead, I enhanced credit and retail applications for diverse financial institutions and implemented recommendation systems for major pharmaceutical companies, utilizing Python, Vertica, and Pentaho.\nKey Contributions and Responsibilities:\n Led a team that preserved FICO\u0026rsquo;s relationship with the nation\u0026rsquo;s largest bank, significantly contributing to client retention Successfully implemented a credit card fraud application, overcoming a year-long delay by another team and demonstrating problem-solving skills and technical proficiency Designed and implemented ETL processes using Pentaho Data Integration, improving data flow and quality Transitioned Kia\u0026rsquo;s application for identifying the optimal dealer and service dealer from VB.Net and PostgreSQL to Java and Vertica, enhancing system performance and reliability Mentored developers in employing Python for diverse ETL techniques, fostering a culture of continuous learning and development Participated in the hiring process, recommending staff for recruitment to strengthen the team   10/2011 – 12/2012\nSenior ETL Architect Productive Data Solutions: Denver, CO In my role as an ETL consultant, I led the design and implementation of effective ETL systems for clients, providing strategic guidance and expert knowledge.\nKey Contributions and Responsibilities:\n Designed and implemented ETL processes utilizing a blend of Pentaho Data Integration and Python, improving data workflow and integration Developed a unique data mapping solution using Django, JQuery, and Oracle, enabling a smooth migration to databases with different source and target schemas Recommended and facilitated the transition of a client\u0026rsquo;s Pentaho repository to a file-based system via subversion, dramatically reducing deployment time from over an hour to a mere 30 seconds Educated developers on diverse ETL techniques utilizing Python, contributing to team skill development Implemented a HIPPA-compliant reporting system using Python, enhancing data security and privacy Mentored QA staff on automation strategies through Linux shell scripting, promoting a culture of continuous learning and innovation Enhanced sprint velocity by 50% by leading an initiative to refine user stories for ETL sprints in collaboration with business analysts and clients Participated in the hiring process, conducting interviews for SQL developers   3/2006 – 10/2011\nSoftware Architect Transzap: Denver, CO As a Software Architect, I was instrumental in enhancing the performance of both customer-facing software products and internal data systems.\nKey Contributions and Responsibilities:\n Pioneered the introduction of Python for efficient data transformations and task automation Developed multiple Python applications to ensure system conversion and upgrade integrity Led the migration of Transzap\u0026rsquo;s legacy e-payables system from Orion to Tomcat Introduced columnar database technology (Vertica) to offload reporting load from the transactional database, optimizing performance Converted SSAS cubes to Vertica, simplifying data access via SQL instead of MDX Crafted Java web services for performing analytical queries against Vertica, returning results as XMLA Implemented several ETL systems using Pentaho Data Integration, enhancing data management processes Contributed to system design and implementation that led to Transzap\u0026rsquo;s recognition in the Deloitte Fast 500 Developed SQL Server Integration Services to streamline data migration into a data warehouse Reduced the start-up time of Tranzap’s Spendwork\u0026rsquo;s C# application from minutes to seconds, significantly improving user experience   7/2000 – 3/2006\nApplication Architect Calpine: Fort Collins, CO As a Senior Applications Architect at Calpine, I established standards and best practices for data warehousing, XML, web services, and service-oriented architecture (SOA). I designed and implemented systems to facilitate performance and efficiency monitoring of Calpine\u0026rsquo;s power plant fleet.\nKey Contributions and Responsibilities:\n Provided architectural oversight to numerous development projects, ensuring optimal technical solutions Played a key role in Calpine\u0026rsquo;s early adoption of Microsoft\u0026rsquo;s .Net technologies, collaborating directly with Microsoft on the C# language Directly contributed to Calpine\u0026rsquo;s 5th place ranking in the InformationWeek Top 100 Innovators (InformationWeek, Sept. 19, 2005 issue) Budgeted projects and set initial project management timelines for projects up to $600K, demonstrating financial acumen Conducted comprehensive reviews of database designs for new systems or enhancements to existing systems Evaluated business intelligence tools, gaining consensus from all information services organizations within Calpine Developed a data warehousing and OLAP application using ASP.Net, SQL Server, SSAS for comparing meter data for natural gas and electric power sales Designed and implemented an OLAP cube for power plant fleet reliability analysis Created a data warehouse to automate reporting from a Maximo inventory system using SQL Server, C#, and DTS Designed, developed, and tested back-end components for real-time nationwide telemetry gathering from power plants Created a data mapping tool using C#, ADO.NET, Oracle, and OSI PI Designed and developed the database maintaining power plant meta-data for the Calpine fleet Modified C++/MFC-based libraries to accommodate varying contract periods for power plants Designed, developed, and created an OLAP server from scratch that cached different periods such as gas days, peaking periods, and off-peak periods Participated in the hiring process, interviewing and recommending software development candidates   12/1999 – 7/2000\nSystems Analyst II City of Thornton: Thornton, CO In my role as a Systems Analyst, I guided MIS staff in the utilization of advanced programming languages and technologies, including C++, COM, MTS, and ASP. My responsibilities also encompassed implementing, maintaining, and upgrading mission-critical systems for the City of Thornton.\nKey Contributions and Responsibilities:\n Participated in the hiring process, conducting interviews and recommending software development candidates Provided expert advice on software purchases and implemented a development life cycle for internal projects, supporting effective project management Provided technical support and maintenance for various systems utilized by the City of Thornton, ensuring seamless operations Continually improved system performance, reliability, and security through regular system upgrades and updates, enhancing organizational efficiency   1/1999 – 12/1999\nInformation Technology Lead VantagePoint Network: Fort Collins, CO As an Information Technology Lead, I was a key contributor to the creation of one of the first web-based agricultural platforms, aiding crop professionals in increasing crop yields while reducing costs and environmental impact.\nKey Contributions and Responsibilities:\n Designed and built an innovative system for collecting GPS-based yield card information, leveraging technologies such as C++, ATL COM, ADO, MTS, MSMQ, Oracle, and SDE Created a system for storing soil test information, demonstrating expertise in utilizing C++ ATL COM objects with ADO, MTS, Oracle, and SDE Implemented a COM object to migrate spatial data in an ESRI SDE Oracle database using C++, ATL COM, and the SDE API, enhancing data management efficiency Built an NT Service with a COM interface to encrypt user id and password information, enhancing system security Designed and implemented a web-based crop record management system using ASP, ADO, and Oracle, improving record-keeping efficiency Streamlined the deployment process between development, test, and production systems by creating installation programs with Wise, VBScript, and MTS package exports Assisted the QA department in developing guidelines for bug reporting, testing, and correction, improving software quality Collaborated with database designers to create a well-structured crop database system Organized and participated in code review sessions, ensuring high-quality, efficient code   2/1995 – 1/1999\nProgrammer/Analyst State Farm Insurance Companies: Bloomington, IL As a Programmer/Analyst, I played a critical role in designing and implementing mission-critical business systems for various insurance products.\nKey Contributions and Responsibilities:\n Designed and built COM objects, integrating third-party insurance software packages with State Farm\u0026rsquo;s legacy systems, enhancing compatibility and efficiency Created an intranet-based application for online policy rating, leveraging technologies such as COM, COM TI, DB2, DHTML, and MTS, improving user experience and policy management Established coding standards for my area, serving as a mentor and resource for analysts in C/C++ and MFC, contributing to improved code quality and team expertise Educated and mentored employees in C++ during after-hours sessions, enhancing team skills and knowledge Developed a system to replicate marketing data for up to 5000 locations using C++, DB2, and MQ Series, enabling efficient data management and access Debugged a third-party MFC C++ application for life insurance illustrations at the vendor\u0026rsquo;s site, ensuring accurate and reliable system performance Enhanced an expert system written in AionDS for pricing auto policies, improving pricing accuracy and policy management Performed tuning and debugging of COBOL applications, ensuring optimal performance and reliability. Collaborated with business analysts to uncover business rules for expert systems, enhancing system functionality and relevance    1/1996 - 12/1996\nC++ Instructor Heartland Community College: Bloomington, IL In my role as a C++ Instructor, I leveraged my expertise in C++ programming to educate students on the language\u0026rsquo;s fundamentals and the principles of object-oriented programming.\nMy comprehensive curriculum focused on the following:\n Understanding and applying analysis and design principles Mastering the creation of classes and objects Implementing effective exception-handling techniques Exploring the concepts of inheritance and polymorphism Applying overloaded operators and understanding their applications Adhering to best practices in C++ programming   5/1993 – 1/1995\nComputer Operator Rockwell Automation Allen – Bradley: Mequon, WI In my role as a Computer Operator, I played an integral part in maintaining and operating mainframe, network, and PC systems. I collaborated closely with systems analysts and system programmers to ensure the smooth running of operations.\nKey Contributions and Responsibilities:\n Actively supporting several successful ISO 9000 audits by diligently maintaining documentation for procedures related to mainframe operations Demonstrating advanced skills in writing JCL and REXX scripts, which were utilized to run programs and conduct backups efficiently Regularly monitoring and providing reports on telecommunication usage and availability to optimize system performance Playing a critical role in the successful implementation of automated scheduling for mainframe applications, contributing significantly to increased efficiency and reliability  Education University of Wisconsin - Milwaukee\nBachelor of Business Administration (Completed in December 1994)\nMajor: Management Information Systems\n","permalink":"https://kurtfehlhauer.com/about/","summary":"Resume/CV Kurt Fehlhauer email: kfehlhauer@pm.me\nSummary I am a software engineering leader with a track record of delivering systems in complex environments.\nExperience 2/2022 - Present\nChief Data Architect Stellantis: Remote As the Chief Data Architect at Stellantis, I spearheaded the creation and currently manage the data platform team, encompassing data, DevOps, and software engineers. My key focus lies in overseeing all vehicle telemetry data for distinguished Stellantis brands, including Maserati and Ram trucks while collaborating with partners to deliver robust data products at an enterprise scale.","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/dataengineering/","summary":"","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/fishing/","summary":"","title":""},{"content":"A collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\n  .image-gallery { overflow: auto; margin-left: -1% !important; } .image-gallery li { float: left; display: block; margin: 0 0 1% 1%; width: 19%; } .image-gallery li a { text-align: center; text-decoration: none !important; color: #777; } .image-gallery li a span { display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0; } .image-gallery li a img { width: 100%; display: block; }                                                                                                            ","permalink":"https://kurtfehlhauer.com/photography/","summary":"A collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\n  .image-gallery { overflow: auto; margin-left: -1% !important; } .image-gallery li { float: left; display: block; margin: 0 0 1% 1%; width: 19%; } .image-gallery li a { text-align: center; text-decoration: none !important; color: #777; } .image-gallery li a span { display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0; } .image-gallery li a img { width: 100%; display: block; }                                                                                                            ","title":""}]