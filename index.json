[{"content":"Rust is gaining increasing recognition as the most loved language in the Stack Overflow developer surveys. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by The RedMonk Programming Language Rankings: January 2023. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.\nThough Rust is undeniably elegant, it diverges from other languages in one significant respect – memory management. Unlike C/C++ or Java/Python Rust does not rely on manual memory allocation or garbage collection. It employs a unique concept called \u0026lsquo;memory ownership\u0026rsquo;, which may initially seem bewildering to engineers more familiar with garbage-collected languages like C#, Java, Python, and Scala, where manual memory management is a foreign concept. Those coming from C/C++ may have an easier journey using Rust because they have a headstart with pointers and references. Becoming comfortable with Rust\u0026rsquo;s approach to memory management is an integral part of the learning curve. While many will find the continuous barking of errors by the Rust compiler to be annoying, those error messages will correct many subtle bugs that other languages just ignore. My advice to those keen on exploring Rust is to persevere through these challenges. Embracing Rust will refine your programming skills. Your perseverance will be rewarded with applications that have no buffer overflows, safe concurrency, and predictable latency.\nThus far, JVM languages like Java and Scala have ruled the big data engineering space, underpinning frameworks like Apache Flink and Spark. The JVM dominance in big data systems is a legacy of infrastructure laid down by Java in the Hadoop and MapReduce era. The JDK big data ecosystem is mature and battle-hardened and may have reached its pinnacle. However, as data sizes skyrocket, the downsides of JVM, such as significant memory consumption and garbage collection pauses, become increasingly evident, especially at scale. These issues trigger multi-level repercussions, leading to increased cloud costs, latency, failure rates, and other problems that data engineers grapple with daily. Rust can help you solve those issues.\nThe foundation for Rust\u0026rsquo;s entry into the data engineering world is already in place. There are available libraries for dealing with Arrow, Parquet, and JSON parsing, high-performing caching libraries like Mocha, and the under-development Polars dataframe library that\u0026rsquo;s written in Rust but can be utilized in Python. Asynchronous runtimes like Tokio and Rayon enable multi-core CPU usage.\nFurthermore, the introduction of Delta Lake RS provides a pathway to incorporate Rust into existing Deltalakes. Powered by the Apache Arrow platform, Delta Lake RS opens opportunities to transfer some workloads away from Spark, although much work is still needed for broader adoption.\nLet\u0026rsquo;s get started by transforming JSON to parquet, a common use case. The JDK has strong support for that use case with even stronger support in Scala. See the previous post Two Scala Libraries Every Data Engineer Should Know. Rust has a framework called Serde which can serialize and deserialize various data formats to and from Rust structs. Serde will handle the conversion between JSON and structs but not Parquet. For that, we will need to use the Parquet and Arrow crates.\nGiven this JSON:\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} I want the data stored in the Parquet file that I\u0026rsquo;m creating to have the following schema:\nmessage arrow_schema { REQUIRED BYTE_ARRAY VIN (STRING); REQUIRED BYTE_ARRAY make (STRING); REQUIRED BYTE_ARRAY model (STRING); REQUIRED INT32 year (INTEGER(16,false)); REQUIRED BYTE_ARRAY owner (STRING); OPTIONAL BOOLEAN isRegistered; } First, we start by creating a Rust struct that we can deserialize the JSON into.\nuse serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize, Debug)] #[allow(non_snake_case)] struct Vechicle { VIN: String, make: String, model: String, year: u16, owner: String, isRegistered: Option\u0026lt;bool\u0026gt;, } Using the attributes Serialize and Deserialize invoke compile time macros that create the boilerplate code to serialize and deserialize between structs and JSON. Rust uses particular casings that the compiler enforces through warnings. The VIN and the isRegistered fields are not in snake case, the attribute allow(non_snake_case) is used to suppress the warning.\nNext, we read a file called vehicles.json containing the above JSON. Deserializing the JSON into Rust structs straight forward.\nlet v: Vechicle = serde_json::from_str(\u0026amp;js)?; Reading in the entire file:\nlet mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = Vec::new(); if let Ok(lines) = read_lines(\u0026#34;vehicles.json\u0026#34;) { for line in lines { if let Ok(js) = line { let v: Vechicle = serde_json::from_str(\u0026amp;js)?; vehicles.push(v); } } } Writing to a Parquet file in Rust currently involves some boilerplate code that is accomplished in three steps. Much of this boilerplate could be abstracted away using Rust macros. Macros are beyond the scope of this post.\n Create arrays for each column.  use arrow::array::{ArrayRef, StringArray}; let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); Create a RecordBatch to hold the column arrays. Notice the use of the Arc( ‘Arc’ stands for ‘Atomically Reference Counted’) type. It is a thread-safe reference-counting pointer. Those new to Rust will eventually want to obtain a cursory understanding of how threading works in Rust. For now, it\u0026rsquo;s just an implementation detail.  use arrow::record_batch::RecordBatch; let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), ... ]) .unwrap(); Use the ArrowWriter to write the record batch to a file. The compression is also set in this step.  use parquet::arrow::arrow_writer::ArrowWriter; use parquet::basic::Compression; let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); The complete write function.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use parquet::basic::Compression; use parquet::file::properties::WriterProperties; use std::sync::Arc; fn write(vehicles: Vec\u0026lt;Vechicle\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let makes = StringArray::from( vehicles .iter() .map(|v| v.make.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let models = StringArray::from( vehicles .iter() .map(|v| v.model.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let years = UInt16Array::from(vehicles.iter().map(|v| v.year).collect::\u0026lt;Vec\u0026lt;u16\u0026gt;\u0026gt;()); let owners = StringArray::from( vehicles .iter() .map(|v| v.owner.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let registrations = BooleanArray::from(vehicles.iter().map(|v| v.isRegistered).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;()); let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), (\u0026#34;make\u0026#34;, Arc::new(makes) as ArrayRef), (\u0026#34;model\u0026#34;, Arc::new(models) as ArrayRef), (\u0026#34;year\u0026#34;, Arc::new(years) as ArrayRef), (\u0026#34;owner\u0026#34;, Arc::new(owners) as ArrayRef), (\u0026#34;isRegistered\u0026#34;, Arc::new(registrations) as ArrayRef), ]) .unwrap(); let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); Ok(()) } Reading a Parquet file is much the same but in reverse.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use arrow::record_batch::RecordBatch; use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder; #[allow(non_snake_case)] fn read() -\u0026gt; Result\u0026lt;()\u0026gt; { let file = File::open(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let arrow_reader = ParquetRecordBatchReaderBuilder::try_new(file).unwrap(); let record_batch_reader = arrow_reader.build().unwrap(); let mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = vec![]; for maybe_batch in record_batch_reader { let record_batch = maybe_batch.unwrap(); let VIN = record_batch .column(0) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let make = record_batch .column(1) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let model = record_batch .column(2) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let year = record_batch .column(3) .as_any() .downcast_ref::\u0026lt;UInt16Array\u0026gt;() .unwrap(); let owner = record_batch .column(4) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let isRegistered = record_batch .column(5) .as_any() .downcast_ref::\u0026lt;BooleanArray\u0026gt;(); for i in 0..record_batch.num_rows() { vehicles.push(Vechicle { VIN: VIN.value(i).to_string(), make: make.value(i).to_string(), model: model.value(i).to_string(), year: year.value(i), owner: owner.value(i).to_string(), isRegistered: isRegistered.map(|a| a.value(i)), }); } } In conclusion, transforming data from JSON to Parquet is a straightforward process requiring only a few Rust crates. As the Rust ecosystem is further developed I expect data engineering tasks to become more commonplace. In my subsequent posts, I\u0026rsquo;ll delve into the nitty-gritty of using other libraries for various data engineering tasks. Stay tuned for more insights into the exciting possibilities that Rust brings to the table.\nThe full source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/exploring-rust-for-data-engineering-part-1/","summary":"Rust is gaining increasing recognition as the most loved language in the Stack Overflow developer surveys. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by The RedMonk Programming Language Rankings: January 2023. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.","title":"Exploring Rust For Data Engineering Part 1"},{"content":"As data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable. In this article, I\u0026rsquo;m going to demonstrate how two Scala libraries can work together to convert JSON to Parquet without using Spark, zio-json and Parquet4s. A working knowledge of functional programming in Scala and Zio is helpful, but not required.\nFor this example, we are going to use JSON data which reflects vehicle ownership and if the vehicle is registered or not. The conversion from JSON to Parquet is a two-step process.\n Load the JSON into a Scala case class using zio-json. Create the Parquet file from the Scala case class.  Our data, notice that we have a field that is optional, isRegistered.\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} Next, we construct a case class called Vehicle. The optional field, isRegistered, is an Option type. In the companion object, we create an implicit decoder.\nimport zio.json.* final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } Then we can decode the JSON with the fromJson[type] function.\ndef decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) In order to turn our JSON into Scala case classes it is a matter of passing each JSON string to decodeJson function.\nimport zio.* import zio.json.* import zio.Console._ object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) yield () def run = program The final step is to take our List of case classes and create a Parquet file from them. We need to do some Hadoop configuration, but you won\u0026rsquo;t need to have the full Hadoop ecosystem installed for this example to work.\nimport com.github.mjakubowski84.parquet4s.{ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet yield () At this point, we will have successfully transformed JSON to Parquet.\nWe can verify our write step was successful by reading back the parquet file that was just created. We read the Parquet functionally and handle closing file resources gracefully. Then we display the contents in the vehicle case classes.\nimport com.github.mjakubowski84.parquet4s.ParquetReader def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) def program = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet yield () The output will look like this.\nVehicle(1A123,foo,bar,2002,John Doe,Some(true)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,Some(false)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,None) Not let us put it all together. So that we can:\n Read the JSON into Scala case classes. Write the case classes out as a parquet file. Display the contents of the parquet file. Perform some cleanup.  import zio.* import zio.json.* import zio.Console._ import java.io.File import com.github.mjakubowski84.parquet4s.{ParquetReader, ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) val cleanUp = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) _ \u0026lt;- ZIO.attemptBlocking(new File(s\u0026#34;${wd}/vehicles.parquet\u0026#34;).delete) yield () def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet _ \u0026lt;- cleanUp yield () def run = program In this recipe, we used two libraries zio-json and Parquet4s to easily create Parquet files from JSON. The source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/two-scala-libraries-every-data-engineer-should-know/","summary":"As data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable.","title":"Two Scala Libraries Every Data Engineer Should Know"},{"content":"Resume/CV Kurt Fehlhauer email: kfehlhauer@pm.me\nSummary I am a software engineering leader with a track record of delivering systems in complex environments.\nExperience 2/2022 - Present\nChief Data Architect Stellantis: Remote As the Chief Data Architect at Stellantis, I spearheaded the creation and currently manage the data platform team, encompassing data, DevOps, and software engineers. My key focus lies in overseeing all vehicle telemetry data for distinguished Stellantis brands, including Maserati and Ram trucks while collaborating with partners to deliver robust data products at an enterprise scale.\nKey Accomplishments and Responsibilities:\n Set the strategic roadmap and vision for enterprise-scale data engineering utilizing Databricks Spark and Delta Lake Orchestrated the consolidation of legacy data systems from two distinct car companies: FCA LLC and PSA Group Constructed a multi-petabyte data platform leveraging Airflow, Kubernetes, and Spark Pioneered the introduction of Spark streaming to process billions of daily records Forecasted and managed a multimillion-dollar data platform budget Successfully migrated data from various on-premise data sources to the cloud Regularly report on status and progress directly to C-suite leadership Champion IT innovation to enhance data accessibility and utility through thoughtful design and technology choices Partnered with data privacy officers to implement effective anonymization strategies, protecting personally identifiable information (PII) Established infrastructure supporting machine learning to enrich the vehicle cockpit experience Continuously exploring technologies like Rust to improve system performance, lower costs, and environmental impact Cultivated a robust engineering culture committed to continuous learning and skill expansion Collaborated with partners to foster innovations in engineering and marketing   5/2019 – 2/2022\nSenior Manager, ETL Activision Publishing: Remote As the Senior Manager of ETL at Activision Publishing, I spearheaded a diverse international team of data engineers. We developed data products and extended support to data scientists and analytics analysts across the company, and I also collaborated with data scientists to execute feature engineering.\nKey Contributions and Responsibilities:\n Managed comprehensive silver/gold level data for the Call of Duty franchise Led a successful migration of ETL processes from Amazon Web Services (AWS) to Google Cloud Platform (GCP), enhancing platform compatibility Developed a Spark-based ETL framework that empowered self-service ETL for many use cases Migrated and normalized a legacy ETL system to streamline various Call of Duty title data, achieving a 50% reduction in data availability wait time Introduced Astronomer Airflow to ease the deployment of Airflow across various data teams at Activision Consolidated and simplified Activision\u0026rsquo;s big data platforms from Qubole/Hive/Presto and Redshift to Databricks Promoted the adoption of MLFlow for data science workflows within the Activision Data Science community Enhanced Spark functionality through user-defined functions and integrated third-party data with Activision\u0026rsquo;s game data for improved insights Developed a third-party data ingestion framework using Cats/ZIO, Circe, and http4s on Kubernetes Implemented GDPR, CCPA, and other privacy measures within the data lake to ensure compliance with data protection regulations Partnered with Activision game studios to convert game artifact data into formats queryable in SparkSQL, retooling small data tools to function within Activision\u0026rsquo;s big data frameworks Improved Diversity, Equity, and Inclusion (DEI) efforts by widening the hiring pipeline for potential candidates Mentored staff in Airflow, Kubernetes, Scala, and Spark, enhancing team capabilities Managed multimillion-dollar contracts, ensuring efficient utilization of resources and delivery of various data products   6/2014 – 4/2019\nLead Database Architect Activision Publishing: Boulder, CO/Remote In my role as Lead Data Engineer at Activision\u0026rsquo;s Central Data Products organization, I championed significant advancements in our analytics, model-building capabilities, and game design support. I introduced the use of Apache Airflow and Spark company-wide, significantly impacting our data management and analytical capabilities.\nKey Contributions and Responsibilities:\n Collaborating with game developers to optimize gameplay elements such as vehicles and weapons in Call of Duty: Black Ops IV through data-driven analytics Collaborated with data scientists to construct models that enhanced gameplay performance Boosted the Play of The Match (PTOM) simulation\u0026rsquo;s performance by 20% Pioneered the introduction and training of Apache Spark at Activision, enabling more efficient data analysis Developed Spark extensions in Scala to handle encoded data Managed the hiring process for data engineers and data scientists, contributing to a highly skilled team Spearheaded the company-wide adoption of Airflow and its migration from DCOS to Kubernetes Oversaw multiple Hive, Presto, and Spark clusters within Qubole Formulated best practices for utilizing big data technologies such as Presto and Apache Spark Established a data pipeline service for capturing prelaunch and beta data for Call of Duty titles Designed ETL processes using Python Pandas dataframes for data ingestion for the Chinese version of Call of Duty Worked closely with Tencent to meet the data needs for Call of Duty Online Implemented a Vertica columnar database to support data from Call of Duty Online   1/2013 – 6/2014\nSenior Consultant FICO: Remote In my role as a team lead, I enhanced credit and retail applications for diverse financial institutions and implemented recommendation systems for major pharmaceutical companies, utilizing Python, Vertica, and Pentaho.\nKey Contributions and Responsibilities:\n Led a team that preserved FICO\u0026rsquo;s relationship with the nation\u0026rsquo;s largest bank, significantly contributing to client retention Successfully implemented a credit card fraud application, overcoming a year-long delay by another team and demonstrating problem-solving skills and technical proficiency Designed and implemented ETL processes using Pentaho Data Integration, improving data flow and quality Transitioned Kia\u0026rsquo;s application for identifying the optimal dealer and service dealer from VB.Net and PostgreSQL to Java and Vertica, enhancing system performance and reliability Mentored developers in employing Python for diverse ETL techniques, fostering a culture of continuous learning and development Participated in the hiring process, recommending staff for recruitment to strengthen the team   10/2011 – 12/2012\nSenior ETL Architect Productive Data Solutions: Denver, CO In my role as an ETL consultant, I led the design and implementation of effective ETL systems for clients, providing strategic guidance and expert knowledge.\nKey Contributions and Responsibilities:\n Designed and implemented ETL processes utilizing a blend of Pentaho Data Integration and Python, improving data workflow and integration Developed a unique data mapping solution using Django, JQuery, and Oracle, enabling a smooth migration to databases with different source and target schemas Recommended and facilitated the transition of a client\u0026rsquo;s Pentaho repository to a file-based system via subversion, dramatically reducing deployment time from over an hour to a mere 30 seconds Educated developers on diverse ETL techniques utilizing Python, contributing to team skill development Implemented a HIPPA-compliant reporting system using Python, enhancing data security and privacy Mentored QA staff on automation strategies through Linux shell scripting, promoting a culture of continuous learning and innovation Enhanced sprint velocity by 50% by leading an initiative to refine user stories for ETL sprints in collaboration with business analysts and clients Participated in the hiring process, conducting interviews for SQL developers   3/2006 – 10/2011\nSoftware Architect Transzap: Denver, CO As a Software Architect, I was instrumental in enhancing the performance of both customer-facing software products and internal data systems.\nKey Contributions and Responsibilities:\n Pioneered the introduction of Python for efficient data transformations and task automation Developed multiple Python applications to ensure system conversion and upgrade integrity Led the migration of Transzap\u0026rsquo;s legacy e-payables system from Orion to Tomcat Introduced columnar database technology (Vertica) to offload reporting load from the transactional database, optimizing performance Converted SSAS cubes to Vertica, simplifying data access via SQL instead of MDX Crafted Java web services for performing analytical queries against Vertica, returning results as XMLA Implemented several ETL systems using Pentaho Data Integration, enhancing data management processes Contributed to system design and implementation that led to Transzap\u0026rsquo;s recognition in the Deloitte Fast 500 Developed SQL Server Integration Services to streamline data migration into a data warehouse Reduced the start-up time of Tranzap’s Spendwork\u0026rsquo;s C# application from minutes to seconds, significantly improving user experience   7/2000 – 3/2006\nApplication Architect Calpine: Fort Collins, CO As a Senior Applications Architect at Calpine, I established standards and best practices for data warehousing, XML, web services, and service-oriented architecture (SOA). I designed and implemented systems to facilitate performance and efficiency monitoring of Calpine\u0026rsquo;s power plant fleet.\nKey Contributions and Responsibilities:\n Provided architectural oversight to numerous development projects, ensuring optimal technical solutions Played a key role in Calpine\u0026rsquo;s early adoption of Microsoft\u0026rsquo;s .Net technologies, collaborating directly with Microsoft on the C# language Directly contributed to Calpine\u0026rsquo;s 5th place ranking in the InformationWeek Top 100 Innovators (InformationWeek, Sept. 19, 2005 issue) Budgeted projects and set initial project management timelines for projects up to $600K, demonstrating financial acumen Conducted comprehensive reviews of database designs for new systems or enhancements to existing systems Evaluated business intelligence tools, gaining consensus from all information services organizations within Calpine Developed a data warehousing and OLAP application using ASP.Net, SQL Server, SSAS for comparing meter data for natural gas and electric power sales Designed and implemented an OLAP cube for power plant fleet reliability analysis Created a data warehouse to automate reporting from a Maximo inventory system using SQL Server, C#, and DTS Designed, developed, and tested back-end components for real-time nationwide telemetry gathering from power plants Created a data mapping tool using C#, ADO.NET, Oracle, and OSI PI Designed and developed the database maintaining power plant meta-data for the Calpine fleet Modified C++/MFC-based libraries to accommodate varying contract periods for power plants Designed, developed, and created an OLAP server from scratch that cached different periods such as gas days, peaking periods, and off-peak periods Participated in the hiring process, interviewing and recommending software development candidates   12/1999 – 7/2000\nSystems Analyst II City of Thornton: Thornton, CO In my role as a Systems Analyst, I guided MIS staff in the utilization of advanced programming languages and technologies, including C++, COM, MTS, and ASP. My responsibilities also encompassed implementing, maintaining, and upgrading mission-critical systems for the City of Thornton.\nKey Contributions and Responsibilities:\n Participated in the hiring process, conducting interviews and recommending software development candidates Provided expert advice on software purchases and implemented a development life cycle for internal projects, supporting effective project management Provided technical support and maintenance for various systems utilized by the City of Thornton, ensuring seamless operations Continually improved system performance, reliability, and security through regular system upgrades and updates, enhancing organizational efficiency   1/1999 – 12/1999\nInformation Technology Lead VantagePoint Network: Fort Collins, CO As an Information Technology Lead, I was a key contributor to the creation of one of the first web-based agricultural platforms, aiding crop professionals in increasing crop yields while reducing costs and environmental impact.\nKey Contributions and Responsibilities:\n Designed and built an innovative system for collecting GPS-based yield card information, leveraging technologies such as C++, ATL COM, ADO, MTS, MSMQ, Oracle, and SDE Created a system for storing soil test information, demonstrating expertise in utilizing C++ ATL COM objects with ADO, MTS, Oracle, and SDE Implemented a COM object to migrate spatial data in an ESRI SDE Oracle database using C++, ATL COM, and the SDE API, enhancing data management efficiency Built an NT Service with a COM interface to encrypt user id and password information, enhancing system security Designed and implemented a web-based crop record management system using ASP, ADO, and Oracle, improving record-keeping efficiency Streamlined the deployment process between development, test, and production systems by creating installation programs with Wise, VBScript, and MTS package exports Assisted the QA department in developing guidelines for bug reporting, testing, and correction, improving software quality Collaborated with database designers to create a well-structured crop database system Organized and participated in code review sessions, ensuring high-quality, efficient code   2/1995 – 1/1999\nProgrammer/Analyst State Farm Insurance Companies: Bloomington, IL As a Programmer/Analyst, I played a critical role in designing and implementing mission-critical business systems for various insurance products.\nKey Contributions and Responsibilities:\n Designed and built COM objects, integrating third-party insurance software packages with State Farm\u0026rsquo;s legacy systems, enhancing compatibility and efficiency Created an intranet-based application for online policy rating, leveraging technologies such as COM, COM TI, DB2, DHTML, and MTS, improving user experience and policy management Established coding standards for my area, serving as a mentor and resource for analysts in C/C++ and MFC, contributing to improved code quality and team expertise Educated and mentored employees in C++ during after-hours sessions, enhancing team skills and knowledge Developed a system to replicate marketing data for up to 5000 locations using C++, DB2, and MQ Series, enabling efficient data management and access Debugged a third-party MFC C++ application for life insurance illustrations at the vendor\u0026rsquo;s site, ensuring accurate and reliable system performance Enhanced an expert system written in AionDS for pricing auto policies, improving pricing accuracy and policy management Performed tuning and debugging of COBOL applications, ensuring optimal performance and reliability. Collaborated with business analysts to uncover business rules for expert systems, enhancing system functionality and relevance    1/1996 - 12/1996\nC++ Instructor Heartland Community College: Bloomington, IL In my role as a C++ Instructor, I leveraged my expertise in C++ programming to educate students on the language\u0026rsquo;s fundamentals and the principles of object-oriented programming.\nMy comprehensive curriculum focused on the following:\n Understanding and applying analysis and design principles Mastering the creation of classes and objects Implementing effective exception-handling techniques Exploring the concepts of inheritance and polymorphism Applying overloaded operators and understanding their applications Adhering to best practices in C++ programming   5/1993 – 1/1995\nComputer Operator Rockwell Automation Allen – Bradley: Mequon, WI In my role as a Computer Operator, I played an integral part in maintaining and operating mainframe, network, and PC systems. I collaborated closely with systems analysts and system programmers to ensure the smooth running of operations.\nKey Contributions and Responsibilities:\n Actively supporting several successful ISO 9000 audits by diligently maintaining documentation for procedures related to mainframe operations Demonstrating advanced skills in writing JCL and REXX scripts, which were utilized to run programs and conduct backups efficiently Regularly monitoring and providing reports on telecommunication usage and availability to optimize system performance Playing a critical role in the successful implementation of automated scheduling for mainframe applications, contributing significantly to increased efficiency and reliability  Education University of Wisconsin - Milwaukee\nBachelor of Business Administration (Completed in December 1994)\nMajor: Management Information Systems\n","permalink":"https://kurtfehlhauer.com/about/","summary":"Resume/CV Kurt Fehlhauer email: kfehlhauer@pm.me\nSummary I am a software engineering leader with a track record of delivering systems in complex environments.\nExperience 2/2022 - Present\nChief Data Architect Stellantis: Remote As the Chief Data Architect at Stellantis, I spearheaded the creation and currently manage the data platform team, encompassing data, DevOps, and software engineers. My key focus lies in overseeing all vehicle telemetry data for distinguished Stellantis brands, including Maserati and Ram trucks while collaborating with partners to deliver robust data products at an enterprise scale.","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/dataengineering/","summary":"","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/fishing/","summary":"","title":""},{"content":"A collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\n  .image-gallery { overflow: auto; margin-left: -1% !important; } .image-gallery li { float: left; display: block; margin: 0 0 1% 1%; width: 19%; } .image-gallery li a { text-align: center; text-decoration: none !important; color: #777; } .image-gallery li a span { display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0; } .image-gallery li a img { width: 100%; display: block; }                                                                                                            ","permalink":"https://kurtfehlhauer.com/photography/","summary":"A collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\n  .image-gallery { overflow: auto; margin-left: -1% !important; } .image-gallery li { float: left; display: block; margin: 0 0 1% 1%; width: 19%; } .image-gallery li a { text-align: center; text-decoration: none !important; color: #777; } .image-gallery li a span { display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0; } .image-gallery li a img { width: 100%; display: block; }                                                                                                            ","title":""}]