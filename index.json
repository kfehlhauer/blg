[{"content":"The Rocket crate provides a lot of web server functionality that is simple to use. It compares well to other web-server backend libraries like Flask in Python. Currently, no Rust framework even registers on the Hacker Rank surveys of favorite web frameworks. That will change. Rocket is easy to use with great documentation and has over 21k stars on GitHub. This is a brief but detailed introduction to using Rocket, by the end of this post you should have a basic understanding of how to get, post, and put using Rocket.\nTo get started with Rocket you need to add it to your Cargo.toml file.\n[package] name = \u0026#34;my-first-rocket\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [dependencies] rocket = \u0026#34;0.5.0-rc.3\u0026#34; Adding methods and routes to your main.rs file.\n#[macro_use] extern crate rocket; #[get(\u0026#34;/\u0026#34;)] fn index() -\u0026gt; \u0026amp;\u0026#39;static str { \u0026#34;Hello, world!\u0026#34; } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![index]) } Then run cargo run and enter http://127.0.0.1:8000/ into your browser. You should see \u0026ldquo;Hello, world!\u0026rdquo;.\nFor comparison, the Python example coded in Flask is comparable to the Rust version coded in Rocket.\nfrom flask import Flask app = Flask(__name__) @app.route(\u0026#34;/\u0026#34;) def index(): return \u0026#34;Hello World!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: app.run() This is about the same amount of code as needed in Python\u0026rsquo;s Flask. With some extra code going towards specifying types and brackets. For a small investment in additional syntax, you gain all the benefits of type safety, no garbage collection, and not having to install Python\u0026rsquo;s runtimes to deploy this code. Creating and deploying the Rocket version is as simple as running cargo build --release and then copying the binary to the deployment location.\nThe get was pretty easy. Next, I will demonstrate the post and put to get a better understanding of what it is like to code using Rocket. This is where life gets a little more interesting. One of the selling points of Rust is \u0026ldquo;fearless concurrency\u0026rdquo; which is not even a thing in frameworks like Flask. Flask also has different goals than Rocket, it doesn\u0026rsquo;t pretend to be highly concurrent. Rocket like Rust wants to make use of all your CPU cores. That comes with the cost of learning how to write concurrent code in Rust.\nIn this demo, I will be replicating a database in a hashmap data structure using HashMap\u0026lt;u32, String\u0026gt;. However, since we need to make this thread safe it will need to be wrapped in a mutex and held in a Rocket State like this.\nuse rocket::tokio::sync::Mutex; use rocket::State; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; The State reference requires a lifetime which is what \u0026lt;'r\u0026gt; syntax represents.\nSo far we have only defined the types that make up the database. We have not instantiated it yet. Later, when we wire up the routes we will instantiate the hashmap and add it as a managed resource. This will make it available to functions using Rocket\u0026rsquo;s annotations.\n#[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![index]) .manage(DataHashMap::new(HashMap::new())) } JSON payloads tend to be ubiquitous on the Web whether it is for REST payloads or other purposes. In this post, we will examine two ways to represent deserialized JSON within a struct. First, the obvious approach. It has the benefit of being easy to understand at the cost of constantly having to own the string when you need to modify the value. The entire point of using Rust is to be efficient, so this is not the most idiomatic approach.\nuse rocket::serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } The more idiomatic approach is to use the Cow smart pointer since you don\u0026rsquo;t have to think about if the value is just borrowed or owned. However, you will need to consider lifetimes in functions that use it. A small penalty to pay for the performance gain. However, depending on your use case. If your endpoint is under a light load, the additional memory spent may be worth it, but then you probably don\u0026rsquo;t need Rust or Rocket either.\nuse rocket::serde::{Deserialize, Serialize}; use std::borrow::Cow; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } First, I will implement the post, put, and get using non-reference versions of the struct.\n#[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![new]) // Added the route for the \u0026#34;new\u0026#34; function .manage(DataHashMap::new(HashMap::new())) } The function new is annotated with the relative path, the format of the payload, JSON in this case and the variable that will hold the payload.\n#[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] Since Rocket is designed to be concurrent we are going to use async functions. The function receives the JSON payload which is deserialized by Serde which is what data: Json\u0026lt;Data\u0026gt; does. The final argument passes in the database which is nothing more than a hashmap wrapped in a mutex that has its state managed by Rocket as declared earlier. The code database: Database\u0026lt;'_\u0026gt; may look odd, this is where we have to use lifetime notation to assist the Rust borrow checker because the type Database is a reference type. This ensures that the reference will remain valid through the life of the function call. The \u0026lt;'_\u0026gt; represents an anonymous lifetime. When you see \u0026lt;\u0026rsquo;_\u0026gt; in a type annotation or a function signature, it means that the reference has a specific, but unnamed, lifetime. The compiler will infer the actual lifetime based on the context where the reference is used. The function then returns a tuple of its status and a detailed message in a returned JSON payload.\nasync fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) Since our database is wrapped in a mutex we must obtain a lock to it, so the state does not change while we attempt to retrieve a value for a given key in the hashmap. The code also checks for a unique key and does not allow duplicate entries for a given id.\nlet mut db = database.lock().await; // Get lock on the hashmap if db.contains_key(\u0026amp;data.id) { // Check for duplicate id! return ( // If a duplicate return with approriate status message Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } Once we are sure we don\u0026rsquo;t have a duplicate entry in our database we do the insert and return success results to the client.\ndb.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) At this point, the complete working example.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![new]) .manage(DataHashMap::new(HashMap::new())) } To retrieve the data stored we need to implement the get function and add its route to the mount. Also, notice the use of pattern matching which makes the code more compact and easier to comprehend.\n#[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new]) .manage(DataHashMap::new(HashMap::new())) } Now we can insert data and return data. Using the VS Code plugin Rest Client\nPOST http://127.0.0.1:8000/insert HTTP/1.1 content-type: application/json { \u0026#34;id\u0026#34;: 1, \u0026#34;value\u0026#34;: \u0026#34;NEW DATA\u0026#34; } To get the data back, in a browser enter http://127.0.0.1:8000/getdata/1 or use the Rest Client.\nGET http://127.0.0.1:8000/getdata/1 HTTP/1.1 The results are:\nHTTP/1.1 202 Accepted content-type: application/json server: Rocket x-content-type-options: nosniff x-frame-options: SAMEORIGIN permissions-policy: interest-cohort=() content-length: 15 date: Tue, 04 Jul 2023 12:58:05 GMT { \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; } The update function using a put will be more of the same.\n#[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } The complete example.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; if db.contains_key(\u0026amp;data.id) { return ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ); } db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } #[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } Next, let\u0026rsquo;s refactor this code to make it more idiomatic Rust. We will replace:\n#[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data { id: u32, value: String, } with\nuse rocket::serde::{Deserialize, Serialize}; use std::borrow::Cow; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } The post will also be refactored to use pattern matching.\nuse rocket::http::Status; use rocket::serde::json::{json, Json, Value}; use rocket::serde::{Deserialize, Serialize}; use rocket::tokio::sync::Mutex; use rocket::State; use std::borrow::Cow; use std::collections::HashMap; #[macro_use] extern crate rocket; type DataHashMap = Mutex\u0026lt;HashMap\u0026lt;u32, String\u0026gt;\u0026gt;; type Database\u0026lt;\u0026#39;r\u0026gt; = \u0026amp;\u0026#39;r State\u0026lt;DataHashMap\u0026gt;; #[derive(Serialize, Deserialize)] #[serde(crate = \u0026#34;rocket::serde\u0026#34;)] struct Data\u0026lt;\u0026#39;r\u0026gt; { id: u32, value: Cow\u0026lt;\u0026#39;r, str\u0026gt;, } #[post(\u0026#34;/insert\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn new(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { let mut db = database.lock().await; match db.contains_key(\u0026amp;data.id) { false =\u0026gt; { db.insert(data.id, data.value.to_string()); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } true =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - record already exists\u0026#34; }), ), } } #[put(\u0026#34;/update\u0026#34;, format = \u0026#34;json\u0026#34;, data = \u0026#34;\u0026lt;data\u0026gt;\u0026#34;)] async fn update(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get_mut(\u0026amp;data.id) { Some(d) =\u0026gt; { *d = data.value.to_string(); (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34; })) } None =\u0026gt; { let status = format!( \u0026#34;failed - record does not exist for {}\u0026#34;, \u0026amp;data.id.to_string() ); (Status::BadRequest, json!({ \u0026#34;status\u0026#34;: status })) } } } #[get(\u0026#34;/getdata/\u0026lt;id\u0026gt;\u0026#34;)] async fn get_my_data(id: u32, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) { match database.lock().await.get(\u0026amp;id) { Some(d) =\u0026gt; (Status::Accepted, json!({ \u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;data\u0026#34;: d })), None =\u0026gt; ( Status::BadRequest, json!({ \u0026#34;status\u0026#34;: \u0026#34;failed - no data found\u0026#34;}), ), } } #[launch] fn rocket() -\u0026gt; _ { rocket::build() .mount(\u0026#34;/\u0026#34;, routes![get_my_data, new, update]) .manage(DataHashMap::new(HashMap::new())) } In the final refactored version we now take advantage of smart pointers, however, now we need to deal with more lifetimes.\nasync fn new(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) // data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt; has lifetime notation async fn update(data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt;, database: Database\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; (Status, Value) // data: Json\u0026lt;Data\u0026lt;\u0026#39;_\u0026gt;\u0026gt; has lifetime notation Conclusion Rocket is simple and it shares a lot in common with other libraries for different languages. Becoming productive in Rust is possible once you overcome the learning curve of the borrow checker and learn some concurrent programming skills. If you already have those skills, using Rocket is just as simple as using frameworks in more dynamic languages. Also, does not take six months to become proficient in Rust, you can become productive in two months.\nRust\u0026rsquo;s ecosystem for web development is growing. It doesn\u0026rsquo;t have complete batteries-included (opinionated) frameworks like Django but that also mirrors the Rust standard runtime which is not prescriptive and encourages the use of the crates that meet your needs. Rocket is one of many frameworks under active development such as Actix Web and Axum(https://github.com/tokio-rs/axum). The \u0026ldquo;Are we web yet?\u0026rdquo; is a great resource for learning about the state of web development in Rust.\nOther thoughts While not readily apparent from this intro to Rocket, managing memory as efficiently as possible will have an impact on performance and cost. Does it matter? It depends on your company\u0026rsquo;s goals and what its values are. Most companies don\u0026rsquo;t care about writing efficient code for the sake of writing efficient code. Most companies value developer productivity more than server costs which is why garbage-collected languages like Python, Go, and Java remain popular. For that class of company, sticking with Python, Javascript, etc. will serve them well. On the other side of the spectrum are a class of companies where scaling out has significant impacts on costs or in some cases all out performance matters to them. Making use of all your cores in Rust is a much lower-risk activity compared to C/C++. A real-world example of this was Cloudflare\u0026rsquo;s rewriting of an NGINX module in Rust.\nThe source code can be found here. ","permalink":"https://kurtfehlhauer.com/posts/rusts-rocket/","summary":"\u003cp\u003eThe \u003ca href=\"https://rocket.rs\"\u003eRocket\u003c/a\u003e \u003ca href=\"https://crates.io/crates/rocket\"\u003ecrate\u003c/a\u003e provides a lot of web server functionality that is simple to use. It compares well to other web-server backend libraries like \u003ca href=\"https://flask.palletsprojects.com\"\u003eFlask\u003c/a\u003e in Python. Currently, no Rust framework even registers on the \u003ca href=\"https://survey.stackoverflow.co/2023/?mod=djemCIO#section-most-popular-technologies-web-frameworks-and-technologies\"\u003eHacker Rank\u003c/a\u003e surveys of favorite web frameworks. That will change. Rocket is easy to use with great documentation and has over 21k stars on GitHub. This is a brief but detailed introduction to using Rocket, by the end of this post you should have a basic understanding of how to get, post, and put using Rocket.\u003c/p\u003e","title":"Rust's Rocket"},{"content":"Rust is gaining increasing recognition as the most loved language in the Stack Overflow developer surveys. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by The RedMonk Programming Language Rankings: January 2023. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.\nThough Rust is undeniably elegant, it diverges from other languages in one significant respect – memory management. Unlike C/C++ or Java/Python Rust does not rely on manual memory allocation or garbage collection. It employs a unique concept called \u0026lsquo;memory ownership\u0026rsquo;, which may initially seem bewildering to engineers more familiar with garbage-collected languages like C#, Java, Python, and Scala, where manual memory management is a foreign concept. Those coming from C/C++ may have an easier journey using Rust because they have a headstart with pointers and references. Becoming comfortable with Rust\u0026rsquo;s approach to memory management is an integral part of the learning curve. While many will find the continuous barking of errors by the Rust compiler to be annoying, those error messages will correct many subtle bugs that other languages just ignore. My advice to those keen on exploring Rust is to persevere through these challenges. Embracing Rust will refine your programming skills. Your perseverance will be rewarded with applications that have no buffer overflows, safe concurrency, and predictable latency.\nThus far, JVM languages like Java and Scala have ruled the big data engineering space, underpinning frameworks like Apache Flink and Spark. The JVM dominance in big data systems is a legacy of infrastructure laid down by Java in the Hadoop and MapReduce era. The JDK big data ecosystem is mature and battle-hardened and may have reached its pinnacle. However, as data sizes skyrocket, the downsides of JVM, such as significant memory consumption and garbage collection pauses, become increasingly evident, especially at scale. These issues trigger multi-level repercussions, leading to increased cloud costs, latency, failure rates, and other problems that data engineers grapple with daily. Rust can help you solve those issues.\nThe foundation for Rust\u0026rsquo;s entry into the data engineering world is already in place. There are available libraries for dealing with Arrow, Parquet, and JSON parsing, high-performing caching libraries like Mocha, and the under-development Polars dataframe library that\u0026rsquo;s written in Rust but can be utilized in Python. Asynchronous runtimes like Tokio and Rayon enable multi-core CPU usage.\nFurthermore, the introduction of Delta Lake RS provides a pathway to incorporate Rust into existing Deltalakes. Powered by the Apache Arrow platform, Delta Lake RS opens opportunities to transfer some workloads away from Spark, although much work is still needed for broader adoption.\nLet\u0026rsquo;s get started by transforming JSON to parquet, a common use case. The JDK has strong support for that use case with even stronger support in Scala. See the previous post Two Scala Libraries Every Data Engineer Should Know. Rust has a framework called Serde which can serialize and deserialize various data formats to and from Rust structs. Serde will handle the conversion between JSON and structs but not Parquet. For that, we will need to use the Parquet and Arrow crates.\nGiven this JSON:\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} I want the data stored in the Parquet file that I\u0026rsquo;m creating to have the following schema:\nmessage arrow_schema { REQUIRED BYTE_ARRAY VIN (STRING); REQUIRED BYTE_ARRAY make (STRING); REQUIRED BYTE_ARRAY model (STRING); REQUIRED INT32 year (INTEGER(16,false)); REQUIRED BYTE_ARRAY owner (STRING); OPTIONAL BOOLEAN isRegistered; } First, we start by creating a Rust struct that we can deserialize the JSON into.\nuse serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize, Debug)] #[allow(non_snake_case)] struct Vechicle { VIN: String, make: String, model: String, year: u16, owner: String, isRegistered: Option\u0026lt;bool\u0026gt;, } Using the attributes Serialize and Deserialize invoke compile time macros that create the boilerplate code to serialize and deserialize between structs and JSON. Rust uses particular casings that the compiler enforces through warnings. The VIN and the isRegistered fields are not in the snake case, the attribute allow(non_snake_case) is used to suppress the warning.\nNext, we read a file called vehicles.json containing the above JSON. Deserializing the JSON into Rust structs straight forward.\nlet v: Vechicle = serde_json::from_str(\u0026amp;js)?; Reading in the entire file:\nlet mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = Vec::new(); if let Ok(lines) = read_lines(\u0026#34;vehicles.json\u0026#34;) { for line in lines { if let Ok(js) = line { let v: Vechicle = serde_json::from_str(\u0026amp;js)?; vehicles.push(v); } } } Writing to a Parquet file in Rust currently involves some boilerplate code that is accomplished in three steps. Much of this boilerplate could be abstracted away using Rust macros. Macros are beyond the scope of this post.\nCreate arrays for each column. use arrow::array::{ArrayRef, StringArray}; let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); Create a RecordBatch to hold the column arrays. Notice the use of the Arc( ‘Arc’ stands for ‘Atomically Reference Counted’) type. It is a thread-safe reference-counting pointer. Those new to Rust will eventually want to obtain a cursory understanding of how threading works in Rust. For now, it\u0026rsquo;s just an implementation detail. use arrow::record_batch::RecordBatch; let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), ... ]) .unwrap(); Use the ArrowWriter to write the record batch to a file. The compression is also set in this step. use parquet::arrow::arrow_writer::ArrowWriter; use parquet::basic::Compression; let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); The complete write function.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use parquet::basic::Compression; use parquet::file::properties::WriterProperties; use std::sync::Arc; fn write(vehicles: Vec\u0026lt;Vechicle\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { let vins = StringArray::from( vehicles .iter() .map(|v| v.VIN.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let makes = StringArray::from( vehicles .iter() .map(|v| v.make.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let models = StringArray::from( vehicles .iter() .map(|v| v.model.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let years = UInt16Array::from(vehicles.iter().map(|v| v.year).collect::\u0026lt;Vec\u0026lt;u16\u0026gt;\u0026gt;()); let owners = StringArray::from( vehicles .iter() .map(|v| v.owner.clone()) .collect::\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;(), ); let registrations = BooleanArray::from(vehicles.iter().map(|v| v.isRegistered).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;()); let batch = RecordBatch::try_from_iter(vec![ (\u0026#34;VIN\u0026#34;, Arc::new(vins) as ArrayRef), (\u0026#34;make\u0026#34;, Arc::new(makes) as ArrayRef), (\u0026#34;model\u0026#34;, Arc::new(models) as ArrayRef), (\u0026#34;year\u0026#34;, Arc::new(years) as ArrayRef), (\u0026#34;owner\u0026#34;, Arc::new(owners) as ArrayRef), (\u0026#34;isRegistered\u0026#34;, Arc::new(registrations) as ArrayRef), ]) .unwrap(); let file = File::create(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let props = WriterProperties::builder() .set_compression(Compression::SNAPPY) .build(); let mut writer = ArrowWriter::try_new(file, batch.schema(), Some(props)).unwrap(); writer.write(\u0026amp;batch).expect(\u0026#34;Unable to write batch\u0026#34;); writer.close().unwrap(); Ok(()) } Reading a Parquet file is much the same but in reverse.\nuse arrow::array::{ArrayRef, BooleanArray, StringArray, UInt16Array}; use arrow::record_batch::RecordBatch; use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder; #[allow(non_snake_case)] fn read() -\u0026gt; Result\u0026lt;()\u0026gt; { let file = File::open(\u0026#34;vehicles.parquet\u0026#34;).unwrap(); let arrow_reader = ParquetRecordBatchReaderBuilder::try_new(file).unwrap(); let record_batch_reader = arrow_reader.build().unwrap(); let mut vehicles: Vec\u0026lt;Vechicle\u0026gt; = vec![]; for maybe_batch in record_batch_reader { let record_batch = maybe_batch.unwrap(); let VIN = record_batch .column(0) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let make = record_batch .column(1) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let model = record_batch .column(2) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let year = record_batch .column(3) .as_any() .downcast_ref::\u0026lt;UInt16Array\u0026gt;() .unwrap(); let owner = record_batch .column(4) .as_any() .downcast_ref::\u0026lt;StringArray\u0026gt;() .unwrap(); let isRegistered = record_batch .column(5) .as_any() .downcast_ref::\u0026lt;BooleanArray\u0026gt;(); for i in 0..record_batch.num_rows() { vehicles.push(Vechicle { VIN: VIN.value(i).to_string(), make: make.value(i).to_string(), model: model.value(i).to_string(), year: year.value(i), owner: owner.value(i).to_string(), isRegistered: isRegistered.map(|a| a.value(i)), }); } } In conclusion, transforming data from JSON to Parquet is a straightforward process requiring only a few Rust crates. As the Rust ecosystem is further developed I expect data engineering tasks to become more commonplace. In my subsequent posts, I\u0026rsquo;ll delve into the nitty-gritty of using other libraries for various data engineering tasks. Stay tuned for more insights into the exciting possibilities that Rust brings to the table.\nThe full source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/exploring-rust-for-data-engineering-part-1/","summary":"\u003cp\u003eRust is gaining increasing recognition as the most loved language in the \u003ca href=\"https://survey.stackoverflow.co/2022#technology-most-loved-dreaded-and-wanted\"\u003eStack Overflow developer surveys\u003c/a\u003e. As such, it\u0026rsquo;s natural to wonder about its potential within the realm of data engineering. Will data engineers begin to love Rust too, or is it just hype? Love versus adoption are two different things as shown by \u003ca href=\"https://redmonk.com/sogrady/2023/05/16/language-rankings-1-23/\"\u003eThe RedMonk Programming Language Rankings: January 2023\u003c/a\u003e. Over the coming weeks and months, I aim to explore and share my insights into the possible trajectories for Rust within this domain.\u003c/p\u003e","title":"Exploring Rust For Data Engineering Part 1"},{"content":"As data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable. In this article, I\u0026rsquo;m going to demonstrate how two Scala libraries can work together to convert JSON to Parquet without using Spark, zio-json and Parquet4s. A working knowledge of functional programming in Scala and Zio is helpful, but not required.\nFor this example, we are going to use JSON data which reflects vehicle ownership and if the vehicle is registered or not. The conversion from JSON to Parquet is a two-step process.\nLoad the JSON into a Scala case class using zio-json. Create the Parquet file from the Scala case class. Our data, notice that we have a field that is optional, isRegistered.\n{\u0026#34;VIN\u0026#34;: \u0026#34;1A123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;year\u0026#34;: 2002, \u0026#34;owner\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;isRegistered\u0026#34;: true} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;, \u0026#34;isRegistered\u0026#34;: false} {\u0026#34;VIN\u0026#34;: \u0026#34;1C123\u0026#34;, \u0026#34;make\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;barV2\u0026#34;, \u0026#34;year\u0026#34;: 2022, \u0026#34;owner\u0026#34;: \u0026#34;John Doe Jr.\u0026#34;} Next, we construct a case class called Vehicle. The optional field, isRegistered, is an Option type. In the companion object, we create an implicit decoder.\nimport zio.json.* final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } Then we can decode the JSON with the fromJson[type] function.\ndef decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) In order to turn our JSON into Scala case classes it is a matter of passing each JSON string to decodeJson function.\nimport zio.* import zio.json.* import zio.Console._ object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) yield () def run = program The final step is to take our List of case classes and create a Parquet file from them. We need to do some Hadoop configuration, but you won\u0026rsquo;t need to have the full Hadoop ecosystem installed for this example to work.\nimport com.github.mjakubowski84.parquet4s.{ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet yield () At this point, we will have successfully transformed JSON to Parquet.\nWe can verify our write step was successful by reading back the parquet file that was just created. We read the Parquet functionally and handle closing file resources gracefully. Then we display the contents in the vehicle case classes.\nimport com.github.mjakubowski84.parquet4s.ParquetReader def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) def program = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet yield () The output will look like this.\nVehicle(1A123,foo,bar,2002,John Doe,Some(true)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,Some(false)) Vehicle(1C123,foo,barV2,2022,John Doe Jr.,None) Not let us put it all together. So that we can:\nRead the JSON into Scala case classes. Write the case classes out as a parquet file. Display the contents of the parquet file. Perform some cleanup. import zio.* import zio.json.* import zio.Console._ import java.io.File import com.github.mjakubowski84.parquet4s.{ParquetReader, ParquetWriter, Path} import org.apache.parquet.hadoop.metadata.CompressionCodecName import org.apache.hadoop.conf.Configuration final case class Vehicle( VIN: String, make: String, model: String, year: Int, owner: String, isRegistered: Option[Boolean] ) object Vehicle { implicit val decoder: JsonDecoder[Vehicle] = DeriveJsonDecoder.gen[Vehicle] } object Main extends zio.ZIOAppDefault: val getData = ZIO.acquireReleaseWith(ZIO.attemptBlocking(io.Source.fromFile(\u0026#34;src/main/resources/vehicles.json\u0026#34;)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attemptBlocking(file.getLines().toList)) def decodeJson(json: String) = ZIO.fromEither(json.fromJson[Vehicle]) val hadoopConf = new Configuration() hadoopConf.set(\u0026#34;fs.s3a.path.style.access\u0026#34;, \u0026#34;true\u0026#34;) val writerOptions = ParquetWriter.Options( compressionCodecName = CompressionCodecName.SNAPPY, hadoopConf = hadoopConf ) def saveAsParquet(vehicles: List[Vehicle]) = ZIO.attemptBlocking( ParquetWriter .of[Vehicle] .options(writerOptions) .writeAndClose(Path(\u0026#34;vehicles.parquet\u0026#34;), vehicles) ) def readParquet(file: Path): Task[List[Vehicle]] = ZIO.acquireReleaseWith(ZIO.attemptBlocking(ParquetReader.as[Vehicle].read(file)))(file =\u0026gt; ZIO.attempt(file.close()).orDie )(file =\u0026gt; ZIO.attempt(file.foldLeft(List[Vehicle]())((acc, vehicle) =\u0026gt; acc :+ vehicle))) val cleanUp = for wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) _ \u0026lt;- ZIO.attemptBlocking(new File(s\u0026#34;${wd}/vehicles.parquet\u0026#34;).delete) yield () def program = for vehicleJson \u0026lt;- getData _ \u0026lt;- ZIO.attempt(vehicleJson.foreach(println)) // Display the raw JSON vehicles \u0026lt;- ZIO.foreach(vehicleJson)(decodeJson) _ \u0026lt;- saveAsParquet(vehicles) // Save to Parquet wd \u0026lt;- ZIO.attempt(java.lang.System.getProperty(\u0026#34;user.dir\u0026#34;)) vehiclesFromParquet \u0026lt;- readParquet(Path(s\u0026#34;${wd}/vehicles.parquet\u0026#34;)) // Read back the data we just saved _ \u0026lt;- ZIO.attempt(vehiclesFromParquet.foreach(println)) // Display the decoded Parquet _ \u0026lt;- cleanUp yield () def run = program In this recipe, we used two libraries zio-json and Parquet4s to easily create Parquet files from JSON. The source code is available here.\n","permalink":"https://kurtfehlhauer.com/posts/two-scala-libraries-every-data-engineer-should-know/","summary":"\u003cp\u003eAs data engineers, we deal with a lot of JSON, which is ubiquitous since JSON is easy for developers to add to applications. However, JSON is not an efficient storage format for applications that frequently query or use the data at scale. Unlike Parquet, JSON is not a splittable file format making it less parallelizable in systems like Spark. Often JSON is not performant enough and requires further ETL to be converted to formats like Parquet which is a splittable file format and therefore parallelizable. In this article, I\u0026rsquo;m going to demonstrate how two Scala libraries can work together to convert JSON to Parquet without using Spark, \u003ca href=\"https://github.com/zio/zio-json\"\u003ezio-json\u003c/a\u003e and \u003ca href=\"https://github.com/mjakubowski84/parquet4s\"\u003eParquet4s\u003c/a\u003e. A working knowledge of functional programming in Scala and Zio is helpful, but not required.\u003c/p\u003e","title":"Two Scala Libraries Every Data Engineer Should Know"},{"content":"Resume/CV Kurt Fehlhauer email: kfehlhauer@pm.me\nSummary A hands-on engineering leader with a proven track record of creating teams that transform visions into systems.\nExperience 2/2022 - Present\nChief Data Architect Stellantis: Remote Spearheaded deparment-wide data architecture transformation at Stellantis, delivering scalable multi-petabyte platforms that enable AI initiatives and data-driven decision making across global operations.\nBuilt and led a global data engineering organization spanning multiple continents, establishing DevOps practices that reduced system delivery time by 40% Architected and deployed a multi-petabyte data platform using Databricks Spark, Kubernetes, and Delta Lake, supporting 500+ employees with enterprise-scale analytics Pioneered implementation of Rust-based query engines (DataFusion, delta-rs) to reduce query costs by 80% for data quality time series data. Successfully consolidated disparate data systems from the FCA LLC and PSA Group merger, eliminating redundancies and reducing operational costs by $2M annually Designed and implemented petabyte-scale vehicle data collection architecture for both research and production fleets, enabling real-time analytics and predictive maintenance Led cross-functional collaboration with AI teams to productionize large language models (LLMs), accelerating time-to-market for AI-powered features Established comprehensive data governance framework and PII handling protocols, ensuring regulatory compliance across multiple regions Forecast yearly multimillion-dollar data platform budge Currently serving as Acting Head of Data Governance (March 2025), focusing on data discovery, AI enablement, and data democratization initiatives Developed portable cloud-agnostic architectures, ensuring business continuity in regions with limited cloud vendor availability 5/2019 – 2/2022\nSenior Manager, ETL Activision Publishing: Remote Led a data engineering team supporting the Call of Duty franchise and game studio analytics, driving platform modernization initiatives that reduced data processing times by 50% while ensuring regulatory compliance across global markets.\nManaged the analytical data pipeline architecture for the Call of Duty franchise, processing petabytes of player behavioral data and game telemetry to support 100M+ active users Orchestrated successful cloud migration from AWS to Google Cloud Platform, reducing infrastructure costs by 30% and improving cross-platform compatibility for development teams Spearheaded legacy ETL system modernization, normalizing data across multiple Call of Duty titles and achieving a 50% reduction in data availability latency Implemented Astronomer Airflow deployment, standardizing workflow orchestration and reducing deployment complexity Consolidated fragmented big data ecosystem (Qubole, Hive, Presto, Redshift) into unified Databricks platform, eliminating vendor sprawl and reducing operational overhead by 40% Championed MLFlow adoption across the data science organization, establishing standardized ML lifecycle management for 50+ data scientists Engineered a high-performance third-party data ingestion framework using functional programming paradigms (Cats/ZIO, Circe, http4s) on Kubernetes, enabling real-time integration of external market data Established a comprehensive data privacy and compliance framework implementing GDPR and CCPA industry regulations, ensuring zero compliance violations across global data operations Built scalable data transformation pipelines converting proprietary game artifacts into Spark SQL-queryable formats, democratizing data access for 200+ analysts and stakeholders Mentored 20+ engineers in modern data stack technologies (Airflow, Kubernetes, Scala, Spark). Managed $5M+ annual vendor contracts and platform investments, optimizing cost-per-query metrics and ensuring 99.9% platform availability 6/2014 – 4/2019\nLead Database Architect Activision Publishing: Boulder, CO/Remote Pioneered game analytics data architecture at Activision, establishing foundational analytics infrastructure that supported billion-dollar gaming franchises and enabled data-driven game design decisions across global development studios.\nArchitected and managed a comprehensive data ecosystem for the Call of Duty franchise, processing petabytes of player telemetry and game performance data supporting 300M+ registered users Led groundbreaking company-wide adoption of Apache Spark and Airflow, establishing Activision as an early adopter of modern big data technologies and reducing processing times by 60% Designed and implemented a self-service Spark-based ETL framework adopted across Call of Duty mainline and Call of Duty Mobile, democratizing data access for 30+ analysts Spearheaded platform consolidation initiative, migrating from fragmented Qubole/Hive/Presto ecosystem to unified Databricks Delta Lake architecture, reducing operational complexity by 80% Established Activision\u0026rsquo;s first MLFlow implementation for data science workflows, standardizing machine learning lifecycle management across 20+ data scientists and accelerating model deployment by 40% Built custom Spark extensions and UDFs that enhanced analytical capabilities, simplifying otherwise complex and redundant SQL Transformed proprietary game artifact data into Spark SQL-compatible formats, enabling cross-studio analytics and reducing time-to-insight for game developers by 70% Modernized legacy studio data tools to integrate with enterprise big data frameworks, eliminating data silos and improving development team productivity Mentored cross-functional teams on big data best practices, creating technical standards that accelerated analytical insight delivery by 60% 1/2013 – 6/2014\nSenior Consultant FICO: Remote Enhanced credit and retail applications for diverse financial institutions and implemented recommendation systems for major pharmaceutical companies, utilizing Python, Vertica, and Pentaho.\nDelivered mission-critical marketing and Voice/SMS gateway systems for Fortune 500 clients, utilizing Python, Vertica, and Pentaho.\nRescued and delivered a voice/SMS gateway system for the nation\u0026rsquo;s largest bank after inheriting a year-long stalled project, preserving $10M+ annual client relationship and preventing potential contract termination of other services. Designed and implemented recommendation engines for major pharmaceutical companies, optimizing drug treatment awareness by 15% Modernized Kia Motors\u0026rsquo; dealer optimization platform, migrating legacy VB.Net/PostgreSQL architecture to high-performance Java/Vertica solution, improving query response times by 80% Designed scalable ETL frameworks using Pentaho Data Integration, processing terabytes of marketing data and reducing data pipeline failures by 60% Mentored 15+ developers in advanced ETL techniques and data processing methodologies Contributed to talent acquisition strategy, interviewing and recommending candidates that strengthened technical capabilities across multiple client engagement teams 10/2011 – 12/2012\nSenior ETL Architect Productive Data Solutions: Denver, CO Provided strategic guidance and expert knowledge that implemented ETL solutions for multiple states to power their healthcare exchanges.\nDesigned and implemented a hybrid ETL architecture combining Pentaho Data Integration with Python automation, processing millions of records daily and reducing data processing errors by 40% Drastically improved deployment processes by architecting a file-based Pentaho repository system with Subversion integration, achieving 99% deployment time reduction (from 60+ minutes to 30 seconds) Developed a HIPAA-compliant healthcare reporting system using Python, ensuring zero privacy violations while enabling real-time analytics for patient records Enhanced development team productivity by 50% through Agile methodology optimization, collaborating with business analysts to refine user story definitions and sprint planning processes Established automated testing framework using Linux shell scripting, mentoring QA teams on DevOps practices that reduced manual testing efforts by 70% Led technical training initiatives for development teams on advanced Python ETL techniques, improving code quality and standardizing best practices across projects Contributed to strategic hiring decisions through technical interviewing of SQL developers, ensuring team capability alignment with client needs Delivered solutions for complex data integration challenges across healthcare, financial services, and retail verticals under tight regulatory constraints Led the design and implementation of effective ETL systems for clients, providing strategic guidance and expert knowledge. 3/2006 – 10/2011\nSoftware Architect Transzap: Denver, CO Drove enterprise software architecture transformation at high-growth oil and gas fintech startup, delivering scalable e-payables solutions that contributed to the company\u0026rsquo;s recognition in Deloitte Fast 500 and supported millions in transaction processing volume.\nPioneered adoption of Python-based automation framework, reducing manual data processing time by 80% and establishing foundation for scalable ETL operations across enterprise systems Architected and executed critical legacy system migration from Orion to Tomcat application server, ensuring zero-downtime transition for customer-facing e-payables platform serving thousands of active users Introduced cutting-edge columnar database technology (Vertica) to the enterprise stack, offloading analytical workloads from transactional systems and improving query performance by 90% Led SSAS cube modernization initiative, migrating complex MDX-based analytics to SQL-accessible Vertica platform, democratizing data access for business analysts Built a comprehensive ETL infrastructure using Pentaho Data Integration, processing financial transactions daily while maintaining 99.9% data accuracy Engineered SQL Server Integration Services pipelines for enterprise data warehouse, enabling real-time business intelligence for executive decision-making Optimized mission-critical C# application (Spendworks) startup performance, achieving a 95% reduction in load times (from minutes to seconds), dramatically improving user adoption rates Designed scalable system architecture supporting the company\u0026rsquo;s rapid growth trajectory, handling increases in transaction volume without performance degradation Mentored development team on emerging technologies and architectural best practices, establishing technical standards that supported company scaling from startup to enterprise 7/2000 – 3/2006\nApplication Architect Calpine: Fort Collins, CO Led enterprise architecture at Fortune 500 energy company, pioneering real-time power plant analytics and telemetry systems that optimized operations across a fleet of natural gas facilities while driving company recognition as InformationWeek Top 100 Innovator.\nSpearheaded architectural oversight for $10M+ portfolio of mission-critical development projects, ensuring technical excellence across enterprise systems Pioneered early adoption of Microsoft .NET technologies at enterprise scale, collaborating directly with Microsoft development teams on C# language evolution and establishing Calpine as an industry technology leader Directly contributed technical innovations that earned Calpine a 5th place ranking in InformationWeek Top 100 Innovators (2005), positioning the company as an energy sector technology pioneer Managed project budgets up to $600K and established PMO standards for timeline management, delivering 95% of projects on time and under budget across the IT portfolio Architected a comprehensive business intelligence platform using ASP.NET, SQL Server, and SSAS, enabling real-time analysis of natural gas and electric power sales data Built enterprise data warehouse integrating Maximo inventory systems using SQL Server, C#, and DTS, automating critical operational reporting and reducing manual processes by 80% Developed custom data mapping platform using C#, ADO.NET, Oracle, and OSI PI, standardizing data integration across heterogeneous industrial control systems Enhanced custom OLAP caching server, optimizing performance for complex energy market calculations (gas days, peaking periods, off-peak analysis) with sub-second response times Enhanced legacy C++/MFC libraries to support dynamic contract period management, improving operational flexibility for diverse power purchase agreements 12/1999 – 7/2000\nSystems Analyst II City of Thornton: Thornton, CO Led municipal technology modernization initiatives for a growing suburban city, implementing mission-critical systems serving 77K+ residents while establishing software development best practices and mentoring technical staff on emerging enterprise technologies.\nArchitected and implemented a comprehensive software development lifecycle (SDLC) framework for municipal IT projects, reducing project delivery times by 30% and establishing quality standards across city departments Provided technical leadership and mentorship to MIS staff on advanced programming technologies, including C++, COM, MTS, and ASP, elevating team capabilities in enterprise application development Led strategic technology procurement processes, evaluating and recommending software solutions for critical municipal operations, including public safety, utilities, and citizen services systems Modernized legacy municipal systems serving police, fire, utilities, and administrative departments, ensuring 99.9% uptime for citizen-facing services and emergency response systems Spearheaded technical hiring initiatives, conducting candidate evaluations and building development team capabilities to support the city\u0026rsquo;s rapid growth and technology advancement needs Collaborated with department heads to align technology solutions with operational requirements, ensuring seamless integration across police dispatch, utilities management, and citizen services platforms Created technical documentation, creating a sustainable IT operations framework for the City of Thornton 1/1999 – 12/1999\nInformation Technology Lead VantagePoint Network: Fort Collins, CO Led software engineering that pioneered the development of a groundbreaking precision agriculture platform, creating one of the industry\u0026rsquo;s first web-based agricultural technology solutions that enabled crop professionals to optimize yields while reducing environmental impact across thousands of farming operations.\nArchitected revolutionary GPS-based yield mapping system using C++, ATL COM, ADO, MTS, MSMQ, and Oracle technologies, enabling real-time precision agriculture data collection that improved crop yields by 15-20% for early adopters Designed a comprehensive soil analysis and management platform integrating C++ ATL COM objects with enterprise Oracle databases, providing farmers with data-driven insights for optimized fertilizer application and soil health monitoring Built an advanced geospatial data migration engine using ESRI SDE APIs and C++, enabling seamless integration of precision agriculture data across heterogeneous farming systems and GIS platforms Created a web-based crop record management system using ASP and Oracle, digitizing paper-based processes and reducing administrative overhead by 60% for agricultural operations Led quality assurance standardization initiative, collaborating with QA teams to establish bug tracking, testing protocols, and code quality standards that reduced production defects by 40% Partnered with database architects to design a scalable agricultural data warehouse supporting complex crop rotation analysis, field mapping, and yield prediction algorithms Facilitated code review processes and mentored development team on emerging web technologies, establishing engineering best practices that supported the company\u0026rsquo;s rapid growth in the AgTech market Contributed to technical innovations that positioned VantagePoint as an early leader in the precision agriculture technology sector 2/1995 – 1/1999\nProgrammer/Analyst State Farm Insurance Companies: Bloomington, IL Delivered enterprise-scale insurance technology solutions for a Fortune 50 company, architecting mission-critical systems that processed millions of policies annually while mentoring development teams and establishing coding standards across business units.\nArchitected a comprehensive COM-based integration framework connecting third-party insurance software with State Farm\u0026rsquo;s mainframe legacy systems, enabling seamless data flow across enterprise applications serving 80M+ policyholders Built pioneering intranet-based policy rating application using COM, COM TI, DB2, DHTML, and MTS technologies, reducing policy quote generation time from hours to minutes and improving agent productivity by 40% Led after-hours technical training initiatives in C++ programming, developing internal expertise Designed high-performance data replication system using C++, DB2, and MQ Series, synchronizing marketing data across 5,000+ agent locations nationwide with 99.9% reliability Provided critical vendor support in debugging a complex MFC C++ life insurance illustration application, ensuring accurate actuarial calculations for a multi-billion dollar life insurance portfolio Enhanced AionDS-based expert system for auto policy pricing, implementing advanced business rules that improved pricing accuracy by 25% and reduced underwriting exceptions Optimized legacy COBOL applications supporting core insurance operations, achieving 60% performance improvements through systematic tuning and debugging initiatives Collaborated with business analysts to translate complex insurance regulations into automated business rules, enabling consistent policy management across all product lines Contributed to digital transformation initiatives that positioned State Farm as a technology leader in the insurance industry during the early web adoption period 1/1996 - 12/1996\nC++ Instructor Heartland Community College: Bloomington, IL Delivered comprehensive object-oriented programming education to 50+ students while maintaining a full-time State Farm position, developing curriculum and teaching methodologies that achieved a 95% student success rate in C++ programming concepts.\nDesigned and delivered a comprehensive C++ curriculum covering advanced object-oriented programming principles, data structures, and software engineering best practices for computer science and engineering students Taught complex programming concepts, including inheritance, polymorphism, exception handling, and operator overloading, to a diverse student population with varying technical backgrounds Developed hands-on laboratory exercises and real-world programming projects that improved student comprehension by 40% compared to traditional lecture-only approaches Mentored students in software design principles, debugging techniques, and industry-standard coding practices, preparing them for internships and entry-level development positions Created teaching materials and practical assignments that bridged academic concepts with industry applications, drawing from my development experience Collaborated with computer science department faculty to align curriculum with industry needs and emerging programming trends Maintained 95% student retention rate through engaging instruction methods and individualized support for struggling learners Applied State Farm enterprise development experience to provide students with practical insights into the commercial software development lifecycle 5/1993 – 1/1995\nComputer Operator Rockwell Automation Allen – Bradley: Mequon, WI Operated mission-critical mainframe systems for Fortune 500 industrial automation leader while contributing to successful ISO 9000 certification initiatives.\nOperated and maintained enterprise mainframe systems processing critical manufacturing data for Rockwell\u0026rsquo;s worldwide industrial automation operations, supporting $2B+ annual revenue Developed advanced JCL and REXX automation scripts that reduced manual backup processes by 75% and eliminated human error in critical data protection workflows Led automation initiative for mainframe application scheduling, designing workflows that increased operational efficiency and reduced after-hours support requirements Supported multiple successful ISO 9000 quality certification audits through meticulous documentation of mainframe operational procedures, ensuring compliance with international manufacturing standards Collaborated with systems analysts and programmers on troubleshooting complex system issues Demonstrated exceptional attention to detail in system documentation and change management Provided technical support during system upgrades and maintenance windows, coordinating with engineering teams to minimize production disruption Built foundational expertise in enterprise system operations and quality management that supported transition into a software engineering career Education University of Wisconsin - Milwaukee\nBachelor of Business Administration (Completed in December 1994)\nMajor: Management Information Systems\n","permalink":"https://kurtfehlhauer.com/about/","summary":"\u003ch1 id=\"resumecv\"\u003eResume/CV\u003c/h1\u003e\n\u003cp\u003eKurt Fehlhauer     \t\t\t\t\t\t\t\t\t\t                                                                                                            \u003cbr\u003e\nemail: \u003ca href=\"mailto:kfehlhauer@pm.me\"\u003ekfehlhauer@pm.me\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eA hands-on engineering leader with a proven track record of creating teams that transform visions into systems.\u003c/p\u003e\n\u003ch2 id=\"experience\"\u003eExperience\u003c/h2\u003e\n\u003cp\u003e2/2022 - Present\u003c/p\u003e\n\u003ch3 id=\"chief-data-architect\"\u003eChief Data Architect\u003c/h3\u003e\n\u003ch4 id=\"stellantis-remote\"\u003eStellantis: Remote\u003c/h4\u003e\n\u003cp\u003eSpearheaded deparment-wide data architecture transformation at Stellantis, delivering scalable multi-petabyte platforms that enable AI initiatives and data-driven decision making across global operations.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuilt and led a global data engineering organization spanning multiple continents, establishing DevOps practices that reduced system delivery time by 40%\u003c/li\u003e\n\u003cli\u003eArchitected and deployed a multi-petabyte data platform using Databricks Spark, Kubernetes, and Delta Lake, supporting 500+ employees with enterprise-scale analytics\u003c/li\u003e\n\u003cli\u003ePioneered implementation of Rust-based query engines (DataFusion, delta-rs) to reduce query costs by 80% for data quality time series data.\u003c/li\u003e\n\u003cli\u003eSuccessfully consolidated disparate data systems from the FCA LLC and PSA Group merger, eliminating redundancies and reducing operational costs by $2M annually\u003c/li\u003e\n\u003cli\u003eDesigned and implemented petabyte-scale vehicle data collection architecture for both research and production fleets, enabling real-time analytics and predictive maintenance\u003c/li\u003e\n\u003cli\u003eLed cross-functional collaboration with AI teams to productionize large language models (LLMs), accelerating time-to-market for AI-powered features\u003c/li\u003e\n\u003cli\u003eEstablished comprehensive data governance framework and PII handling protocols, ensuring regulatory compliance across multiple regions\u003c/li\u003e\n\u003cli\u003eForecast yearly multimillion-dollar data platform budge\u003c/li\u003e\n\u003cli\u003eCurrently serving as Acting Head of Data Governance (March 2025), focusing on data discovery, AI enablement, and data democratization initiatives\u003c/li\u003e\n\u003cli\u003eDeveloped portable cloud-agnostic architectures, ensuring business continuity in regions with limited cloud vendor availability\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e5/2019 – 2/2022\u003c/p\u003e","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/dataengineering/","summary":"","title":""},{"content":"","permalink":"https://kurtfehlhauer.com/fishing/","summary":"","title":""},{"content":"A collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\n","permalink":"https://kurtfehlhauer.com/photography/","summary":"\u003cp\u003eA collection of stock photography. Some of which can be licensed through Alamy and/or Shutterstock.\u003c/p\u003e\n\u003cscript type=\"text/javascript\" src=\"/js/lightbox.js\"\u003e\u003c/script\u003e\n\u003clink rel=\"stylesheet\" href=\"/css/lightbox.css\"\u003e\n\n\u003cstyle\u003e\n  .image-gallery {\n    overflow: auto;\n    margin-left: -1% !important;\n  }\n\n  .image-gallery li {\n    float: left;\n    display: block;\n    margin: 0 0 1% 1%;\n    width: 19%;\n  }\n\n  .image-gallery li a {\n    text-align: center;\n    text-decoration: none !important;\n    color: #777;\n  }\n\n  .image-gallery li a span {\n    display: block;\n    text-overflow: ellipsis;\n    overflow: hidden;\n    white-space: nowrap;\n    padding: 3px 0;\n  }\n\n  .image-gallery li a img {\n    width: 100%;\n    display: block;\n  }\n\u003c/style\u003e\n\u003cul class=\"image-gallery\"\u003e\n  \n\n  \n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0001kurtfehlhauer_hu_632bc5cbbbd72029.jpg\" title=\"img/0001kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0001kurtfehlhauer_hu_323da2b5ec1e5595.jpg\" alt=\"img/0001kurtfehlhauer\" title=\"img/0001kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0002kurtfehlhauer_hu_db7d5bed53b6aa3b.jpg\" title=\"img/0002kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0002kurtfehlhauer_hu_ce4ecccea5c550d1.jpg\" alt=\"img/0002kurtfehlhauer\" title=\"img/0002kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0003kurtfehlhauer_hu_376d926a1210f963.jpg\" title=\"img/0003kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0003kurtfehlhauer_hu_a546585ab8d2db04.jpg\" alt=\"img/0003kurtfehlhauer\" title=\"img/0003kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0004kurtfehlhauer_hu_dbbf1bf94fa4e65c.jpg\" title=\"img/0004kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0004kurtfehlhauer_hu_69c66bae8d712568.jpg\" alt=\"img/0004kurtfehlhauer\" title=\"img/0004kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0005kurtfehlhauer_hu_1ad1aca71063a422.jpg\" title=\"img/0005kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0005kurtfehlhauer_hu_5f464b8b90ea5be1.jpg\" alt=\"img/0005kurtfehlhauer\" title=\"img/0005kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0006kurtfehlhauer_hu_98bdcc423cc94148.jpg\" title=\"img/0006kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0006kurtfehlhauer_hu_fcdd5a5bac0ab78f.jpg\" alt=\"img/0006kurtfehlhauer\" title=\"img/0006kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0007kurtfehlhauer_hu_f71d146e6763fdcf.jpg\" title=\"img/0007kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0007kurtfehlhauer_hu_6aae3b60d5bbcf30.jpg\" alt=\"img/0007kurtfehlhauer\" title=\"img/0007kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0008kurtfehlhauer_hu_a78ebc500482592d.jpg\" title=\"img/0008kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0008kurtfehlhauer_hu_955290af64878603.jpg\" alt=\"img/0008kurtfehlhauer\" title=\"img/0008kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0009kurtfehlhauer_hu_b35598c8257d6b69.jpg\" title=\"img/0009kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0009kurtfehlhauer_hu_e573f95a1079b393.jpg\" alt=\"img/0009kurtfehlhauer\" title=\"img/0009kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0010kurtfehlhauer_hu_1c9fc9011d987c55.jpg\" title=\"img/0010kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0010kurtfehlhauer_hu_d0249642bb419409.jpg\" alt=\"img/0010kurtfehlhauer\" title=\"img/0010kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0011kurtfehlhauer_hu_31fc6dc1e595cb87.jpg\" title=\"img/0011kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0011kurtfehlhauer_hu_87b748729591a2c1.jpg\" alt=\"img/0011kurtfehlhauer\" title=\"img/0011kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0012kurtfehlhauer_hu_2ccee594de0621da.jpg\" title=\"img/0012kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0012kurtfehlhauer_hu_e5289f947cf878c.jpg\" alt=\"img/0012kurtfehlhauer\" title=\"img/0012kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0013kurtfehlhauer_hu_95b18f0096dcd4da.jpg\" title=\"img/0013kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0013kurtfehlhauer_hu_6172b5f6ca192d69.jpg\" alt=\"img/0013kurtfehlhauer\" title=\"img/0013kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0014kurtfehlhauer_hu_21396e7c37cc74ac.jpg\" title=\"img/0014kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0014kurtfehlhauer_hu_d064ce13aff00f63.jpg\" alt=\"img/0014kurtfehlhauer\" title=\"img/0014kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0015kurtfehlhauer_hu_e4113c57d7fc1c1.jpg\" title=\"img/0015kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0015kurtfehlhauer_hu_c84bf48444488093.jpg\" alt=\"img/0015kurtfehlhauer\" title=\"img/0015kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0016kurtfehlhauer_hu_f5638ef8818bc2dd.jpg\" title=\"img/0016kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0016kurtfehlhauer_hu_54e4270ee3f4618d.jpg\" alt=\"img/0016kurtfehlhauer\" title=\"img/0016kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0017kurtfehlhauer_hu_7474d7e2edf71fa6.jpg\" title=\"img/0017kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0017kurtfehlhauer_hu_a07b1e7c512cdd53.jpg\" alt=\"img/0017kurtfehlhauer\" title=\"img/0017kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0018kurtfehlhauer_hu_77021fdcea091d94.jpg\" title=\"img/0018kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0018kurtfehlhauer_hu_6b26abe7da4572cb.jpg\" alt=\"img/0018kurtfehlhauer\" title=\"img/0018kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0019kurtfehlhauer_hu_7705306de932c9b.jpg\" title=\"img/0019kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0019kurtfehlhauer_hu_da2ce7e65ff84331.jpg\" alt=\"img/0019kurtfehlhauer\" title=\"img/0019kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0020kurtfehlhauer_hu_4968f9c5a5aabd2f.jpg\" title=\"img/0020kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0020kurtfehlhauer_hu_7774921701fe3e68.jpg\" alt=\"img/0020kurtfehlhauer\" title=\"img/0020kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0021kurtfehlhauer_hu_1f65150989e0a638.jpg\" title=\"img/0021kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0021kurtfehlhauer_hu_222204a4edadf922.jpg\" alt=\"img/0021kurtfehlhauer\" title=\"img/0021kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0022kurtfehlhauer_hu_fb1ff52421b2ec9b.jpg\" title=\"img/0022kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0022kurtfehlhauer_hu_c1d4381d173759e3.jpg\" alt=\"img/0022kurtfehlhauer\" title=\"img/0022kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0023kurtfehlhauer_hu_6cdd2b47428a7282.jpg\" title=\"img/0023kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0023kurtfehlhauer_hu_8de0239c60865910.jpg\" alt=\"img/0023kurtfehlhauer\" title=\"img/0023kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0024kurtfehlhauer_hu_679c2d4762bea200.jpg\" title=\"img/0024kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0024kurtfehlhauer_hu_3fae47405b131ca2.jpg\" alt=\"img/0024kurtfehlhauer\" title=\"img/0024kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0025kurtfehlhauer_hu_6285ed66d1df42d4.jpg\" title=\"img/0025kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0025kurtfehlhauer_hu_bc393f7600077369.jpg\" alt=\"img/0025kurtfehlhauer\" title=\"img/0025kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0026kurtfehlhauer_hu_67326adc5eb2a409.jpg\" title=\"img/0026kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0026kurtfehlhauer_hu_87ef0af5151589e3.jpg\" alt=\"img/0026kurtfehlhauer\" title=\"img/0026kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0027kurtfehlhauer_hu_c347b0a7fab9f1a3.jpg\" title=\"img/0027kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0027kurtfehlhauer_hu_34c23b506c5d44b9.jpg\" alt=\"img/0027kurtfehlhauer\" title=\"img/0027kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0028kurtfehlhauer_hu_d6a1a41a48365b31.jpg\" title=\"img/0028kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0028kurtfehlhauer_hu_7a86755fa9791f4a.jpg\" alt=\"img/0028kurtfehlhauer\" title=\"img/0028kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0029kurtfehlhauer_hu_e74096af2124871e.jpg\" title=\"img/0029kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0029kurtfehlhauer_hu_c390b120d6a11452.jpg\" alt=\"img/0029kurtfehlhauer\" title=\"img/0029kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0030kurtfehlhauer_hu_2d186ad401507e34.jpg\" title=\"img/0030kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0030kurtfehlhauer_hu_d4825c1ecb58aeb8.jpg\" alt=\"img/0030kurtfehlhauer\" title=\"img/0030kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0031kurtfehlhauer_hu_8dbe3eaf54b41f8c.jpg\" title=\"img/0031kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0031kurtfehlhauer_hu_4f146b6d6f70f97a.jpg\" alt=\"img/0031kurtfehlhauer\" title=\"img/0031kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0032kurtfehlhauer_hu_5ade3de5ee84dec.jpg\" title=\"img/0032kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0032kurtfehlhauer_hu_8d423c77bf65f3c7.jpg\" alt=\"img/0032kurtfehlhauer\" title=\"img/0032kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0033kurtfehlhauer_hu_a4d866b5342c54a6.jpg\" title=\"img/0033kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0033kurtfehlhauer_hu_c354ca0d5e355ddd.jpg\" alt=\"img/0033kurtfehlhauer\" title=\"img/0033kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0034kurtfehlhauer_hu_eea7364dff55f74b.jpg\" title=\"img/0034kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0034kurtfehlhauer_hu_8f1b1b873d4fd849.jpg\" alt=\"img/0034kurtfehlhauer\" title=\"img/0034kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n  \u003cli\u003e\n    \u003ca href=\"https://kurtfehlhauer.com/photography/img/0035kurtfehlhauer_hu_a66ca111b39d4640.jpg\" title=\"img/0035kurtfehlhauer\" class=\"lightbox-image\"\u003e\n      \u003cimg src=\"https://kurtfehlhauer.com/photography/img/0035kurtfehlhauer_hu_59ee3f006c449295.jpg\" alt=\"img/0035kurtfehlhauer\" title=\"img/0035kurtfehlhauer\"\u003e\n    \u003c/a\u003e\n  \u003c/li\u003e\n  \n  \n\u003c/ul\u003e","title":""}]